{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6525169,"sourceType":"datasetVersion","datasetId":3772434},{"sourceId":7174585,"sourceType":"datasetVersion","datasetId":4145795}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# preprocessing of images","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nimport os\nimport shutil\nimport re\nimport math\nimport matplotlib.pyplot as plt\nimport skimage\nfrom skimage.io import imread, imsave\nfrom skimage.transform import resize\nfrom skimage import img_as_float32, img_as_ubyte\n\nimport cv2\n\nfrom albumentations import (\n    Compose, HorizontalFlip, ShiftScaleRotate, ElasticTransform,\n    RandomBrightness, RandomContrast, RandomGamma\n)\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n  except RuntimeError as e:\n    print(e)\n%matplotlib inline\nroot_folder = \"/kaggle/input/paper-dataset/covid19-segmentation-paper-main/4_images\"\nmasks_folder = os.path.join(root_folder, \"masks\")\n\nimg_size = 300","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:18:28.531742Z","iopub.execute_input":"2023-12-11T13:18:28.532146Z","iopub.status.idle":"2023-12-11T13:18:28.544597Z","shell.execute_reply.started":"2023-12-11T13:18:28.532113Z","shell.execute_reply":"2023-12-11T13:18:28.542898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = []\nlabels = []\nmanual_images = []\nv7labs_images = []\n\nfor mask_path in os.listdir(masks_folder):\n  mask = imread(os.path.join(masks_folder, mask_path))\n  mask = np.float32(mask / 255)\n  labels.append(mask)\n  target, source, pathogen, pid, offset, _ = re.split(\"[_.]\", mask_path)\n  img_path = \"%s_%s_%s_%s.png\" % (source, pathogen, pid, offset)\n  img = imread(os.path.join(root_folder, target, pathogen, img_path))\n  img = img_as_float32(img)\n  \n  if source == \"Cohen\":\n    v7labs_images.append(img)\n  else:\n    manual_images.append(img)\n  \n  images.append(img)\n  \nlen(labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:18:31.579275Z","iopub.execute_input":"2023-12-11T13:18:31.579659Z","iopub.status.idle":"2023-12-11T13:18:43.545018Z","shell.execute_reply.started":"2023-12-11T13:18:31.579629Z","shell.execute_reply":"2023-12-11T13:18:43.544265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show some of our own CXR and masks\nf = plt.figure()\nf.add_subplot(3, 2, 1)\nplt.imshow(images[50], cmap = \"gray\")\nf.add_subplot(3, 2, 2)\nplt.imshow(labels[50], cmap = \"gray\")\nf.add_subplot(3, 2, 3)\nplt.imshow(images[20], cmap = \"gray\")\nf.add_subplot(3, 2, 4)\nplt.imshow(labels[20], cmap = \"gray\")\nf.add_subplot(3, 2, 5)\nplt.imshow(images[30], cmap = \"gray\")\nf.add_subplot(3, 2, 6)\nplt.imshow(labels[30], cmap = \"gray\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:18:43.546460Z","iopub.execute_input":"2023-12-11T13:18:43.546932Z","iopub.status.idle":"2023-12-11T13:18:44.249688Z","shell.execute_reply.started":"2023-12-11T13:18:43.546904Z","shell.execute_reply":"2023-12-11T13:18:44.248792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(images).reshape((len(images),300,300,-1))\nprint(X.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:25:23.820842Z","iopub.execute_input":"2023-12-11T13:25:23.821221Z","iopub.status.idle":"2023-12-11T13:25:23.979008Z","shell.execute_reply.started":"2023-12-11T13:25:23.821190Z","shell.execute_reply":"2023-12-11T13:25:23.978067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(images).reshape((len(images), img_size,img_size, -1))\nY = np.array(labels).reshape((len(labels), img_size, img_size, -1))\nX, Y = shuffle(X, Y, random_state = 1234)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.05, random_state = 1234)\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.05, random_state = 1234)\n\nprint(X_train.shape)\nprint(X_val.shape)\nprint(X_test.shape)\nprint(X.shape)\n\n# X_train = np.array(X_train).reshape(len(X_train),dim,dim,-1)\n# y_train = np.array(y_train).reshape(len(y_train),dim,dim,-1)\n# X_test = np.array(X_test).reshape(len(X_test),dim,dim,-1)\n# y_test = np.array(y_test).reshape(len(y_test),dim,dim,-1)\n# assert X_train.shape == y_train.shape\n# assert X_test.shape == y_test.shape\n# images = np.concatenate((X_train,X_test),axis=0)\n# mask  = np.concatenate((y_train,y_test),axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:19:02.155033Z","iopub.execute_input":"2023-12-11T13:19:02.155404Z","iopub.status.idle":"2023-12-11T13:19:03.140061Z","shell.execute_reply.started":"2023-12-11T13:19:02.155371Z","shell.execute_reply":"2023-12-11T13:19:03.138972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shenzhen_test_ids = []\njsrt_test_ids = []\nmontgomery_test_ids = []\nv7labs_test_ids = []\nother_test_ids = []\n\nnimages = X_test.shape[0]\nfor idx in range(nimages):\n  test_image = X_test[idx,:,:,0]\n  if any(np.array_equal(test_image, x) for x in shenzhen_images):\n    shenzhen_test_ids.append(idx)\n  elif any(np.array_equal(test_image, x) for x in montgomery_images):\n    montgomery_test_ids.append(idx)\n  elif any(np.array_equal(test_image, x) for x in jsrt_images):\n    jsrt_test_ids.append(idx)\n  elif any(np.array_equal(test_image, x) for x in v7labs_images):\n    v7labs_test_ids.append(idx)\n  else:\n    other_test_ids.append(idx)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:58:58.322181Z","iopub.execute_input":"2023-12-11T12:58:58.323216Z","iopub.status.idle":"2023-12-11T12:58:58.387475Z","shell.execute_reply.started":"2023-12-11T12:58:58.323151Z","shell.execute_reply":"2023-12-11T12:58:58.385262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AugmentationSequence(keras.utils.Sequence):\n  def __init__(self, x_set, y_set, batch_size, augmentations):\n    self.x, self.y = x_set, y_set\n    self.batch_size = batch_size\n    self.augment = augmentations\n\n  def __len__(self):\n    return int(np.ceil(len(self.x) / float(self.batch_size)))\n\n  def __getitem__(self, idx):\n    batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n    batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n    \n    aug_x = np.zeros(batch_x.shape)\n    aug_y = np.zeros(batch_y.shape)\n    \n    for idx in range(batch_x.shape[0]):\n      aug = self.augment(image = batch_x[idx,:,:,:], mask = batch_y[idx,:,:,:])\n      aug_x[idx,:,:,:] = aug[\"image\"]\n      aug_y[idx,:,:,:] = aug[\"mask\"]\n    \n    return aug_x, aug_y\n\naugment = Compose([\n  HorizontalFlip(),\n  ShiftScaleRotate(rotate_limit = 45, border_mode = cv2.BORDER_CONSTANT),\n  ElasticTransform(border_mode = cv2.BORDER_CONSTANT),\n  RandomBrightness(),\n  RandomContrast(),\n  RandomGamma()\n])\n\nbatch_size = 16\ntrain_generator = AugmentationSequence(X_train, Y_train, batch_size, augment)\nsteps_per_epoch = math.ceil(X_train.shape[0] / batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:19:17.029809Z","iopub.execute_input":"2023-12-11T13:19:17.030198Z","iopub.status.idle":"2023-12-11T13:19:17.045344Z","shell.execute_reply.started":"2023-12-11T13:19:17.030164Z","shell.execute_reply":"2023-12-11T13:19:17.044452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_aug, Y_aug = train_generator.__getitem__(20)\n\nf = plt.figure()\nf.add_subplot(4, 2, 1)\nplt.imshow(X_aug[0,:,:,0], cmap = \"gray\")\nf.add_subplot(4, 2, 2)\nplt.imshow(Y_aug[0,:,:,0], cmap = \"gray\")\n\nf.add_subplot(4, 2, 3)\nplt.imshow(X_aug[1,:,:,0], cmap = \"gray\")\nf.add_subplot(4, 2, 4)\nplt.imshow(Y_aug[1,:,:,0], cmap = \"gray\")\n\nf.add_subplot(4, 2, 5)\nplt.imshow(X_aug[2,:,:,0], cmap = \"gray\")\nf.add_subplot(4, 2, 6)\nplt.imshow(Y_aug[2,:,:,0], cmap = \"gray\")\n\nf.add_subplot(4, 2, 7)\nplt.imshow(X_aug[3,:,:,0], cmap = \"gray\")\nf.add_subplot(4, 2, 8)\nplt.imshow(Y_aug[3,:,:,0], cmap = \"gray\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:19:20.375919Z","iopub.execute_input":"2023-12-11T13:19:20.377164Z","iopub.status.idle":"2023-12-11T13:19:22.246606Z","shell.execute_reply.started":"2023-12-11T13:19:20.377120Z","shell.execute_reply":"2023-12-11T13:19:22.245847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOSS Functions\ndef jaccard_distance_loss(y_true, y_pred, smooth = 100):\n    intersection = keras.backend.sum(keras.backend.abs(y_true * y_pred), axis = -1)\n    union = keras.backend.sum(keras.backend.abs(y_true) + keras.backend.abs(y_pred), axis = -1)\n    jac = (intersection + smooth) / (union - intersection + smooth)\n    loss = (1 - jac) * smooth\n    return loss\n\ndef dice_coef(y_true, y_pred, smooth = 1):\n    intersection = keras.backend.sum(keras.backend.abs(y_true * y_pred), axis = -1)\n    union = keras.backend.sum(keras.backend.abs(y_true), -1) + keras.backend.sum(keras.backend.abs(y_pred), -1)\n    return (2. * intersection + smooth) / (union + smooth)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:19:25.085888Z","iopub.execute_input":"2023-12-11T13:19:25.087722Z","iopub.status.idle":"2023-12-11T13:19:25.097912Z","shell.execute_reply.started":"2023-12-11T13:19:25.087670Z","shell.execute_reply":"2023-12-11T13:19:25.095446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pdb\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport random\nclass AxialBlock_dynamic(nn.Module):\n  expansion = 2\n\n  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                base_width=64, dilation=1, norm_layer=None, kernel_size=56):\n      super(AxialBlock_dynamic, self).__init__()\n      if norm_layer is None:\n          norm_layer = nn.BatchNorm2d\n      width = int(planes * (base_width / 64.))\n      # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n      self.conv_down = conv1x1(inplanes, width)\n      self.bn1 = norm_layer(width)\n      self.hight_block = AxialAttention_dynamic(width, width, groups=groups, kernel_size=kernel_size)\n      self.width_block = AxialAttention_dynamic(width, width, groups=groups, kernel_size=kernel_size, stride=stride, width=True)\n      self.conv_up = conv1x1(width, planes * self.expansion)\n      self.bn2 = norm_layer(planes * self.expansion)\n      self.relu = nn.ReLU(inplace=True)\n      self.downsample = downsample\n      self.stride = stride\n\n  def forward(self, x):\n      identity = x\n\n      out = self.conv_down(x)\n      out = self.bn1(out)\n      out = self.relu(out)\n\n      out = self.hight_block(out)\n      out = self.width_block(out)\n      out = self.relu(out)\n\n      out = self.conv_up(out)\n      out = self.bn2(out)\n\n      if self.downsample is not None:\n          identity = self.downsample(x)\n      #print(out.shape)\n      #print(identity.shape)\n      out += identity\n      out = self.relu(out)\n\n      return out\nclass ResAxialAttentionUNet(nn.Module):\n\n  def __init__(self, block, layers, num_classes=2, zero_init_residual=True,\n                groups=8, width_per_group=64, replace_stride_with_dilation=None,\n                norm_layer=None, s=0.125, img_size = 128,imgchan = 3):\n      super(ResAxialAttentionUNet, self).__init__()\n      if norm_layer is None:\n          norm_layer = nn.BatchNorm2d\n      self._norm_layer = norm_layer\n\n      self.inplanes = int(64 * s)\n      self.dilation = 1\n      if replace_stride_with_dilation is None:\n          replace_stride_with_dilation = [False, False, False]\n      if len(replace_stride_with_dilation) != 3:\n          raise ValueError(\"replace_stride_with_dilation should be None \"\n                            \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n      self.groups = groups\n      self.base_width = width_per_group\n      self.conv1 = nn.Conv2d(imgchan, self.inplanes, kernel_size=7, stride=2, padding=3,\n                              bias=False)\n      self.conv2 = nn.Conv2d(self.inplanes, 128, kernel_size=3, stride=1, padding=1, bias=False)\n      self.conv3 = nn.Conv2d(128, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n      self.bn1 = norm_layer(self.inplanes)\n      self.bn2 = norm_layer(128)\n      self.bn3 = norm_layer(self.inplanes)\n      self.relu = nn.ReLU(inplace=True)\n      # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n      self.layer1 = self._make_layer(block, int(128 * s), layers[0], kernel_size= (img_size//2))\n      self.layer2 = self._make_layer(block, int(256 * s), layers[1], stride=2, kernel_size=(img_size//2),\n                                      dilate=replace_stride_with_dilation[0])\n      self.layer3 = self._make_layer(block, int(512 * s), layers[2], stride=2, kernel_size=(img_size//4),\n                                      dilate=replace_stride_with_dilation[1])\n      self.layer4 = self._make_layer(block, int(1024 * s), layers[3], stride=2, kernel_size=(img_size//8),\n                                      dilate=replace_stride_with_dilation[2])\n      \n      # Decoder\n      self.decoder1 = nn.Conv2d(int(1024 *2*s)      ,        int(1024*2*s), kernel_size=3, stride=2, padding=1)\n      self.decoder2 = nn.Conv2d(int(1024  *2*s)     , int(1024*s), kernel_size=3, stride=1, padding=1)\n      self.decoder3 = nn.Conv2d(int(1024*s),  int(512*s), kernel_size=3, stride=1, padding=1)\n      self.decoder4 = nn.Conv2d(int(512*s) ,  int(256*s), kernel_size=3, stride=1, padding=1)\n      self.decoder5 = nn.Conv2d(int(256*s) , int(128*s) , kernel_size=3, stride=1, padding=1)\n      self.adjust   = nn.Conv2d(int(128*s) , num_classes, kernel_size=1, stride=1, padding=0)\n      self.soft     = nn.Softmax(dim=1)\n\n\n  def _make_layer(self, block, planes, blocks, kernel_size=56, stride=1, dilate=False):\n      norm_layer = self._norm_layer\n      downsample = None\n      previous_dilation = self.dilation\n      if dilate:\n          self.dilation *= stride\n          stride = 1\n      if stride != 1 or self.inplanes != planes * block.expansion:\n          downsample = nn.Sequential(\n              conv1x1(self.inplanes, planes * block.expansion, stride),\n              norm_layer(planes * block.expansion),\n          )\n\n      layers = []\n      layers.append(block(self.inplanes, planes, stride, downsample, groups=self.groups,\n                          base_width=self.base_width, dilation=previous_dilation, \n                          norm_layer=norm_layer, kernel_size=kernel_size))\n      self.inplanes = planes * block.expansion\n      if stride != 1:\n          kernel_size = kernel_size // 2\n\n      for _ in range(1, blocks):\n          layers.append(block(self.inplanes, planes, groups=self.groups,\n                              base_width=self.base_width, dilation=self.dilation,\n                              norm_layer=norm_layer, kernel_size=kernel_size))\n\n      return nn.Sequential(*layers)\n\n  def _forward_impl(self, x):\n      \n      # AxialAttention Encoder\n      # pdb.set_trace()\n      x = self.conv1(x)\n      x = self.bn1(x)\n      x = self.relu(x)\n      x = self.conv2(x)\n      x = self.bn2(x)\n      x = self.relu(x)\n      x = self.conv3(x)\n      x = self.bn3(x)\n      x = self.relu(x)\n      #print(x.shape)\n      x1 = self.layer1(x)\n      #print(x.shape)\n      x2 = self.layer2(x1)\n      # print(x2.shape)\n      x3 = self.layer3(x2)\n      # print(x3.shape)\n      x4 = self.layer4(x3)\n\n      x = F.relu(F.interpolate(self.decoder1(x4), scale_factor=(2,2), mode ='bilinear'))\n      x = torch.add(x, x4)\n      x = F.relu(F.interpolate(self.decoder2(x) , scale_factor=(2,2), mode ='bilinear'))\n      x = torch.add(x, x3)\n      x = F.relu(F.interpolate(self.decoder3(x) , scale_factor=(2,2), mode ='bilinear'))\n      x = torch.add(x, x2)\n      x = F.relu(F.interpolate(self.decoder4(x) , scale_factor=(2,2), mode ='bilinear'))\n      x = torch.add(x, x1)\n      x = F.relu(F.interpolate(self.decoder5(x) , scale_factor=(2,2), mode ='bilinear'))\n      x = self.adjust(F.relu(x))\n      # pdb.set_trace()\n      return x\n\n  def forward(self, x):\n      return self._forward_impl(x)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:32:03.069563Z","iopub.execute_input":"2023-12-11T13:32:03.069992Z","iopub.status.idle":"2023-12-11T13:32:03.104464Z","shell.execute_reply.started":"2023-12-11T13:32:03.069957Z","shell.execute_reply":"2023-12-11T13:32:03.102667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gated(pretrained=False, **kwargs):\n  model = ResAxialAttentionUNet(AxialBlock_dynamic, [1, 2, 4, 1], s= 0.125, **kwargs)\n  return model","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:32:08.188276Z","iopub.execute_input":"2023-12-11T13:32:08.188651Z","iopub.status.idle":"2023-12-11T13:32:08.193517Z","shell.execute_reply.started":"2023-12-11T13:32:08.188624Z","shell.execute_reply":"2023-12-11T13:32:08.192574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = gated()\nx_test = torch.rand(1, 3, 128, 128)\nnet(x_test);","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:32:10.880613Z","iopub.execute_input":"2023-12-11T13:32:10.882352Z","iopub.status.idle":"2023-12-11T13:32:11.326086Z","shell.execute_reply.started":"2023-12-11T13:32:10.882307Z","shell.execute_reply":"2023-12-11T13:32:11.324379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, Add, BatchNormalization, Conv2DTranspose, Softmax\nfrom tensorflow.keras.models import Model\n\ndef conv1x1(in_channels, out_channels, stride=1):\n    return Conv2D(out_channels, (1, 1), strides=(stride, stride), padding='same', use_bias=False)\n\nclass AxialAttentionBlock(tf.keras.layers.Layer):\n    def __init__(self, in_channels, out_channels, kernel_size=150, stride=1):\n        super(AxialAttentionBlock, self).__init__()\n        self.conv1 = Conv2D(out_channels, (1, kernel_size), strides=(1, stride), padding='same', use_bias=False)\n        self.conv2 = Conv2D(out_channels, (kernel_size, 1), strides=(stride, 1), padding='same', use_bias=False)\n        self.conv3 = Conv2D(out_channels, (1, 1), padding='same', use_bias=False)\n        self.batch_norm1 = BatchNormalization()\n        self.batch_norm2 = BatchNormalization()\n        self.batch_norm3 = BatchNormalization()\n        self.relu = Activation('relu')\n\n    def call(self, x):\n        x1 = self.relu(self.batch_norm1(self.conv1(x)))\n        x2 = self.relu(self.batch_norm2(self.conv2(x)))\n        x = self.relu(self.batch_norm3(self.conv3(x)))\n        return Add()([x1, x2, x])\n\ndef axial_attention_unet(input_shape=(300, 300, 1), num_classes=2):\n    inputs = Input(shape=input_shape)\n    \n    # Encoder\n    x = Conv2D(64, kernel_size=7, strides=2, padding='same', use_bias=False)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Conv2D(128, kernel_size=3, strides=1, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Conv2D(64, kernel_size=3, strides=1, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x1 = AxialAttentionBlock(64, 128, kernel_size=input_shape[0] // 2)(x)\n    x2 = AxialAttentionBlock(128, 256, kernel_size=input_shape[0] // 2)(x1)\n    x3 = AxialAttentionBlock(256, 512, kernel_size=input_shape[0] // 4)(x2)\n    x4 = AxialAttentionBlock(512, 1024, kernel_size=input_shape[0] // 8)(x3)\n\n    # Decoder\n    x = Conv2DTranspose(1024, kernel_size=3, strides=2, padding='same')(x4)\n    x = Add()([x, x4])\n    x = Conv2DTranspose(1024, kernel_size=3, strides=1, padding='same')(x)\n    x = Add()([x, x3])\n    x = Conv2DTranspose(512, kernel_size=3, strides=1, padding='same')(x)\n    x = Add()([x, x2])\n    x = Conv2DTranspose(256, kernel_size=3, strides=1, padding='same')(x)\n    x = Add()([x, x1])\n    x = Conv2DTranspose(128, kernel_size=3, strides=1, padding='same')(x)\n    x = Add()([x, x1])\n    x = Conv2DTranspose(num_classes, kernel_size=1, strides=1, padding='same')(x)\n    x = Softmax(axis=-1)(x)\n\n    model = Model(inputs=inputs, outputs=x)\n    return model\n\n# Create the model\nmodel = axial_attention_unet(input_shape=(300, 300, 1), num_classes=2)\n\n# Print model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:20:34.233520Z","iopub.execute_input":"2023-12-11T13:20:34.234026Z","iopub.status.idle":"2023-12-11T13:20:34.966364Z","shell.execute_reply.started":"2023-12-11T13:20:34.233982Z","shell.execute_reply":"2023-12-11T13:20:34.964494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unet_model():\n  \n  input_img = keras.layers.Input((img_size, img_size, 1), name = \"img\")\n  \n  # Contract #1\n  c1 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(input_img)\n  c1 = keras.layers.BatchNormalization()(c1)\n  c1 = keras.layers.Activation(\"relu\")(c1)\n  c1 = keras.layers.Dropout(0.1)(c1)\n  c1 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c1)\n  c1 = keras.layers.BatchNormalization()(c1)\n  c1 = keras.layers.Activation(\"relu\")(c1)\n  p1 = keras.layers.MaxPooling2D((2, 2))(c1)\n  \n  # Contract #2\n  c2 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p1)\n  c2 = keras.layers.BatchNormalization()(c2)\n  c2 = keras.layers.Activation(\"relu\")(c2)\n  c2 = keras.layers.Dropout(0.2)(c2)\n  c2 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c2)\n  c2 = keras.layers.BatchNormalization()(c2)\n  c2 = keras.layers.Activation(\"relu\")(c2)\n  p2 = keras.layers.MaxPooling2D((2, 2))(c2)\n  \n  # Contract #3\n  c3 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p2)\n  c3 = keras.layers.BatchNormalization()(c3)\n  c3 = keras.layers.Activation(\"relu\")(c3)\n  c3 = keras.layers.Dropout(0.3)(c3)\n  c3 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c3)\n  c3 = keras.layers.BatchNormalization()(c3)\n  c3 = keras.layers.Activation(\"relu\")(c3)\n  p3 = keras.layers.MaxPooling2D((2, 2))(c3)\n  \n  # Contract #4\n  c4 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p3)\n  c4 = keras.layers.BatchNormalization()(c4)\n  c4 = keras.layers.Activation(\"relu\")(c4)\n  c4 = keras.layers.Dropout(0.4)(c4)\n  c4 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c4)\n  c4 = keras.layers.BatchNormalization()(c4)\n  c4 = keras.layers.Activation(\"relu\")(c4)\n  p4 = keras.layers.MaxPooling2D((2, 2))(c4)\n  \n  # Middle\n  c5 = keras.layers.Conv2D(256, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p4)\n  c5 = keras.layers.BatchNormalization()(c5)\n  c5 = keras.layers.Activation(\"relu\")(c5)\n  c5 = keras.layers.Dropout(0.5)(c5)\n  c5 = keras.layers.Conv2D(256, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c5)\n  c5 = keras.layers.BatchNormalization()(c5)\n  c5 = keras.layers.Activation(\"relu\")(c5)\n  \n  # Expand (upscale) #1\n  u6 = keras.layers.Conv2DTranspose(128, (3, 3), strides = (2, 2), padding = \"same\")(c5)\n  u6 = keras.layers.concatenate([u6, c4])\n  c6 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u6)\n  c6 = keras.layers.BatchNormalization()(c6)\n  c6 = keras.layers.Activation(\"relu\")(c6)\n  c6 = keras.layers.Dropout(0.5)(c6)\n  c6 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c6)\n  c6 = keras.layers.BatchNormalization()(c6)\n  c6 = keras.layers.Activation(\"relu\")(c6)\n  \n  # Expand (upscale) #2\n  u7 = keras.layers.Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = \"same\")(c6)\n  u7 = keras.layers.concatenate([u7, c3])\n  c7 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u7)\n  c7 = keras.layers.BatchNormalization()(c7)\n  c7 = keras.layers.Activation(\"relu\")(c7)\n  c7 = keras.layers.Dropout(0.5)(c7)\n  c7 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c7)\n  c7 = keras.layers.BatchNormalization()(c7)\n  c7 = keras.layers.Activation(\"relu\")(c7)\n  \n  # Expand (upscale) #3\n  u8 = keras.layers.Conv2DTranspose(32, (3, 3), strides = (2, 2), padding = \"same\")(c7)\n  u8 = keras.layers.concatenate([u8, c2])\n  c8 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u8)\n  c8 = keras.layers.BatchNormalization()(c8)\n  c8 = keras.layers.Activation(\"relu\")(c8)\n  c8 = keras.layers.Dropout(0.5)(c8)\n  c8 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c8)\n  c8 = keras.layers.BatchNormalization()(c8)\n  c8 = keras.layers.Activation(\"relu\")(c8)\n  \n  # Expand (upscale) #4\n  u9 = keras.layers.Conv2DTranspose(16, (3, 3), strides = (2, 2), padding = \"same\")(c8)\n  u9 = keras.layers.concatenate([u9, c1])\n  c9 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u9)\n  c9 = keras.layers.BatchNormalization()(c9)\n  c9 = keras.layers.Activation(\"relu\")(c9)\n  c9 = keras.layers.Dropout(0.5)(c9)\n  c9 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c9)\n  c9 = keras.layers.BatchNormalization()(c9)\n  c9 = keras.layers.Activation(\"relu\")(c9)\n  \n  output = keras.layers.Conv2D(1, (1, 1), activation = \"sigmoid\")(c9)\n  model = keras.Model(inputs = [input_img], outputs = [output])\n  return model","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:00:28.492514Z","iopub.execute_input":"2023-12-11T13:00:28.492906Z","iopub.status.idle":"2023-12-11T13:00:28.520329Z","shell.execute_reply.started":"2023-12-11T13:00:28.492872Z","shell.execute_reply":"2023-12-11T13:00:28.518680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResAxialAttentionUNet(nn.Module):\n\n  def __init__(self, block, layers, num_classes=2, zero_init_residual=True,\n                groups=8, width_per_group=64, replace_stride_with_dilation=None,\n                norm_layer=None, s=0.125, img_size = 128,imgchan = 3):\n      super(ResAxialAttentionUNet, self).__init__()\n      if norm_layer is None:\n          norm_layer = nn.BatchNorm2d\n      self._norm_layer = norm_layer\n\n      self.inplanes = int(64 * s)\n      self.dilation = 1\n      if replace_stride_with_dilation is None:\n          replace_stride_with_dilation = [False, False, False]\n      if len(replace_stride_with_dilation) != 3:\n          raise ValueError(\"replace_stride_with_dilation should be None \"\n                            \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n      self.groups = groups\n      self.base_width = width_per_group\n      self.conv1 = nn.Conv2d(imgchan, self.inplanes, kernel_size=7, stride=2, padding=3,\n                              bias=False)\n      self.conv2 = nn.Conv2d(self.inplanes, 128, kernel_size=3, stride=1, padding=1, bias=False)\n      self.conv3 = nn.Conv2d(128, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n      self.bn1 = norm_layer(self.inplanes)\n      self.bn2 = norm_layer(128)\n      self.bn3 = norm_layer(self.inplanes)\n      self.relu = nn.ReLU(inplace=True)\n      # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n      self.layer1 = self._make_layer(block, int(128 * s), layers[0], kernel_size= (img_size//2))\n      self.layer2 = self._make_layer(block, int(256 * s), layers[1], stride=2, kernel_size=(img_size//2),\n                                      dilate=replace_stride_with_dilation[0])\n      self.layer3 = self._make_layer(block, int(512 * s), layers[2], stride=2, kernel_size=(img_size//4),\n                                      dilate=replace_stride_with_dilation[1])\n      self.layer4 = self._make_layer(block, int(1024 * s), layers[3], stride=2, kernel_size=(img_size//8),\n                                      dilate=replace_stride_with_dilation[2])\n      \n      # Decoder\n      self.decoder1 = nn.Conv2d(int(1024 *2*s)      ,        int(1024*2*s), kernel_size=3, stride=2, padding=1)\n      self.decoder2 = nn.Conv2d(int(1024  *2*s)     , int(1024*s), kernel_size=3, stride=1, padding=1)\n      self.decoder3 = nn.Conv2d(int(1024*s),  int(512*s), kernel_size=3, stride=1, padding=1)\n      self.decoder4 = nn.Conv2d(int(512*s) ,  int(256*s), kernel_size=3, stride=1, padding=1)\n      self.decoder5 = nn.Conv2d(int(256*s) , int(128*s) , kernel_size=3, stride=1, padding=1)\n      self.adjust   = nn.Conv2d(int(128*s) , num_classes, kernel_size=1, stride=1, padding=0)\n      self.soft     = nn.Softmax(dim=1)\n\n\n  def _make_layer(self, block, planes, blocks, kernel_size=56, stride=1, dilate=False):\n      norm_layer = self._norm_layer\n      downsample = None\n      previous_dilation = self.dilation\n      if dilate:\n          self.dilation *= stride\n          stride = 1\n      if stride != 1 or self.inplanes != planes * block.expansion:\n          downsample = nn.Sequential(\n              conv1x1(self.inplanes, planes * block.expansion, stride),\n              norm_layer(planes * block.expansion),\n          )\n\n      layers = []\n      layers.append(block(self.inplanes, planes, stride, downsample, groups=self.groups,\n                          base_width=self.base_width, dilation=previous_dilation, \n                          norm_layer=norm_layer, kernel_size=kernel_size))\n      self.inplanes = planes * block.expansion\n      if stride != 1:\n          kernel_size = kernel_size // 2\n\n      for _ in range(1, blocks):\n          layers.append(block(self.inplanes, planes, groups=self.groups,\n                              base_width=self.base_width, dilation=self.dilation,\n                              norm_layer=norm_layer, kernel_size=kernel_size))\n\n      return nn.Sequential(*layers)\n\n  def _forward_impl(self, x):\n      \n      # AxialAttention Encoder\n      # pdb.set_trace()\n      x = self.conv1(x)\n      x = self.bn1(x)\n      x = self.relu(x)\n      x = self.conv2(x)\n      x = self.bn2(x)\n      x = self.relu(x)\n      x = self.conv3(x)\n      x = self.bn3(x)\n      x = self.relu(x)\n      #print(x.shape)\n      x1 = self.layer1(x)\n      #print(x.shape)\n      x2 = self.layer2(x1)\n      # print(x2.shape)\n      x3 = self.layer3(x2)\n      # print(x3.shape)\n      x4 = self.layer4(x3)\n\n      x = F.relu(F.interpolate(self.decoder1(x4), scale_factor=(2,2), mode ='bilinear'))\n      x = torch.add(x, x4)\n      x = F.relu(F.interpolate(self.decoder2(x) , scale_factor=(2,2), mode ='bilinear'))\n      x = torch.add(x, x3)\n      x = F.relu(F.interpolate(self.decoder3(x) , scale_factor=(2,2), mode ='bilinear'))\n      x = torch.add(x, x2)\n      x = F.relu(F.interpolate(self.decoder4(x) , scale_factor=(2,2), mode ='bilinear'))\n      x = torch.add(x, x1)\n      x = F.relu(F.interpolate(self.decoder5(x) , scale_factor=(2,2), mode ='bilinear'))\n      x = self.adjust(F.relu(x))\n      # pdb.set_trace()\n      return x\n\n  def forward(self, x):\n      return self._forward_impl(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_files = set(os.listdir(image_path)) & set(os.listdir(mask_path))\ntraining_files = check\n\ndef getData(X_shape, flag = \"test\"):\n    im_array = []\n    mask_array = []\n   \n    if flag == \"test\":\n        for i in tqdm(testing_files): \n            im = cv2.resize(cv2.imread(os.path.join(image_path,i)),(X_shape,X_shape)),(224,224)[:,:,0]\n            mask = cv2.resize(cv2.imread(os.path.join(mask_path,i)),(X_shape,X_shape)),(224,224)[:,:,0]\n            \n            im_array.append(im)\n            mask_array.append(mask)\n        \n        return im_array,mask_array\n    \n    if flag == \"train\":\n        for i in tqdm(training_files): \n            im = cv2.resize(cv2.imread(os.path.join(image_path,i.split(\"_mask\")[0]+\".png\")),(X_shape,X_shape))[:,:,0]\n            mask = cv2.resize(cv2.imread(os.path.join(mask_path,i+\".png\")),(X_shape,X_shape))[:,:,0]\n\n            im_array.append(im)\n            mask_array.append(mask)\n\n        return im_array,mask_array","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:01:13.902844Z","iopub.execute_input":"2023-12-11T11:01:13.903351Z","iopub.status.idle":"2023-12-11T11:01:13.919150Z","shell.execute_reply.started":"2023-12-11T11:01:13.903313Z","shell.execute_reply":"2023-12-11T11:01:13.917842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotMask(X,y):\n    sample = []\n    \n    for i in range(6):\n        left = X[i]\n        right = y[i]\n        combined = np.hstack((left,right))\n        sample.append(combined)\n        \n        \n    for i in range(0,6,3):\n\n        plt.figure(figsize=(25,10))\n        \n        plt.subplot(2,3,1+i)\n        plt.imshow(sample[i])\n        \n        plt.subplot(2,3,2+i)\n        plt.imshow(sample[i+1])\n        \n        \n        plt.subplot(2,3,3+i)\n        plt.imshow(sample[i+2])\n        \n        plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:01:34.716911Z","iopub.execute_input":"2023-12-11T11:01:34.718424Z","iopub.status.idle":"2023-12-11T11:01:34.729159Z","shell.execute_reply.started":"2023-12-11T11:01:34.718374Z","shell.execute_reply":"2023-12-11T11:01:34.727369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim = 224\nX_train,y_train = getData(224,flag=\"train\")\nX_test, y_test = getData(224)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:10:41.126153Z","iopub.execute_input":"2023-12-11T11:10:41.127490Z","iopub.status.idle":"2023-12-11T11:13:38.124829Z","shell.execute_reply.started":"2023-12-11T11:10:41.127435Z","shell.execute_reply":"2023-12-11T11:13:38.123106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"training set\")\nplotMask(X_train,y_train)\nprint(\"testing set\")\nplotMask(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:08:51.040736Z","iopub.execute_input":"2023-12-11T11:08:51.041110Z","iopub.status.idle":"2023-12-11T11:08:54.776578Z","shell.execute_reply.started":"2023-12-11T11:08:51.041055Z","shell.execute_reply":"2023-12-11T11:08:54.774278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.array(X_train).reshape(len(X_train),dim,dim,-1)\ny_train = np.array(y_train).reshape(len(y_train),dim,dim,-1)\nX_test = np.array(X_test).reshape(len(X_test),dim,dim,-1)\ny_test = np.array(y_test).reshape(len(y_test),dim,dim,-1)\nassert X_train.shape == y_train.shape\nassert X_test.shape == y_test.shape\nimages = np.concatenate((X_train,X_test),axis=0)\nmask  = np.concatenate((y_train,y_test),axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:13:44.607007Z","iopub.execute_input":"2023-12-11T11:13:44.607528Z","iopub.status.idle":"2023-12-11T11:13:44.738564Z","shell.execute_reply.started":"2023-12-11T11:13:44.607484Z","shell.execute_reply":"2023-12-11T11:13:44.736881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras import backend as keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\n\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = keras.flatten(y_true)\n    y_pred_f = keras.flatten(y_pred)\n    intersection = keras.sum(y_true_f * y_pred_f)\n    return (2. * intersection + 1) / (keras.sum(y_true_f) + keras.sum(y_pred_f) + 1)\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\ndef gated_axial_unet(input_size=(256, 256, 1)):\n    inputs = Input(input_size)\n\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n\n    # Use axial_attention_block instead of Conv2DTranspose for the decoder part\n    up6 = concatenate([axial_attention_block(conv5, 256, 2, 8), conv4], axis=3)\n    up6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n    up6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n\n    up7 = concatenate([axial_attention_block(up6, 128, 2, 16), conv3], axis=3)\n    up7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n    up7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n\n    up8 = concatenate([axial_attention_block(up7, 64, 2, 32), conv2], axis=3)\n    up8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n    up8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n\n    up9 = concatenate([axial_attention_block(up8, 32, 2, 64), conv1], axis=3)\n    up9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n    up9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n\n    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(up9)\n\n    #Model(inputs=[inputs], outputs=[conv10])\n    model = keras.Model(inputs = [input_img], outputs = [conv10])\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:26:50.365122Z","iopub.execute_input":"2023-12-11T14:26:50.365542Z","iopub.status.idle":"2023-12-11T14:26:50.389814Z","shell.execute_reply.started":"2023-12-11T14:26:50.365507Z","shell.execute_reply":"2023-12-11T14:26:50.387830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:26:53.684969Z","iopub.execute_input":"2023-12-11T14:26:53.685365Z","iopub.status.idle":"2023-12-11T14:26:53.726793Z","shell.execute_reply.started":"2023-12-11T14:26:53.685332Z","shell.execute_reply":"2023-12-11T14:26:53.724692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_learning_rate = keras.callbacks.ReduceLROnPlateau(\n  monitor = \"loss\", \n  factor = 0.5, \n  patience = 3, \n  verbose = 1\n)\n\ncheckpointer = keras.callbacks.ModelCheckpoint(\n  \"unet.h5\", \n  verbose = 1, \n  save_best_only = True\n)\n\nstrategy = tf.distribute.MirroredStrategy()\n\nif (os.path.exists(\"unet.h5\")):\n  model = keras.models.load_model(\"unet.h5\",\n    custom_objects = {\n      \"jaccard_distance_loss\": jaccard_distance_loss,\n      \"dice_coef\": dice_coef\n    }\n  )\n  \nelse:\n  with strategy.scope():\n    model = unet_model()\n    adam_opt = keras.optimizers.Adam(learning_rate = 0.001)\n    model.compile(optimizer = adam_opt, loss = jaccard_distance_loss, metrics = [dice_coef])\n    \n  fit = model.fit(train_generator, \n    steps_per_epoch = steps_per_epoch, \n    epochs = 100,\n    validation_data = (X_val, Y_val),\n    callbacks = [\n      checkpointer,\n      reduce_learning_rate\n    ]\n  )","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:24:19.713649Z","iopub.execute_input":"2023-12-11T14:24:19.714113Z","iopub.status.idle":"2023-12-11T14:24:19.763930Z","shell.execute_reply.started":"2023-12-11T14:24:19.714078Z","shell.execute_reply":"2023-12-11T14:24:19.761743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# medt architecture","metadata":{}},{"cell_type":"code","source":"import pdb\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n#from .utils import *\nimport pdb\nimport matplotlib.pyplot as plt\nimport random\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass AxialAttention(nn.Module):\n    def __init__(self, in_planes, out_planes, groups=8, kernel_size=56,\n                 stride=1, bias=False, width=False):\n        assert (in_planes % groups == 0) and (out_planes % groups == 0)\n        super(AxialAttention, self).__init__()\n        self.in_planes = in_planes\n        self.out_planes = out_planes\n        self.groups = groups\n        self.group_planes = out_planes // groups\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.bias = bias\n        self.width = width\n\n        # Multi-head self attention\n        self.qkv_transform = qkv_transform(in_planes, out_planes * 2, kernel_size=1, stride=1,\n                                           padding=0, bias=False)\n        self.bn_qkv = nn.BatchNorm1d(out_planes * 2)\n        self.bn_similarity = nn.BatchNorm2d(groups * 3)\n\n        self.bn_output = nn.BatchNorm1d(out_planes * 2)\n\n        # Position embedding\n        self.relative = nn.Parameter(torch.randn(self.group_planes * 2, kernel_size * 2 - 1), requires_grad=True)\n        query_index = torch.arange(kernel_size).unsqueeze(0)\n        key_index = torch.arange(kernel_size).unsqueeze(1)\n        relative_index = key_index - query_index + kernel_size - 1\n        self.register_buffer('flatten_index', relative_index.view(-1))\n        if stride > 1:\n            self.pooling = nn.AvgPool2d(stride, stride=stride)\n\n        self.reset_parameters()\n\n    def forward(self, x):\n        # pdb.set_trace()\n        if self.width:\n            x = x.permute(0, 2, 1, 3)\n        else:\n            x = x.permute(0, 3, 1, 2)  # N, W, C, H\n        N, W, C, H = x.shape\n        x = x.contiguous().view(N * W, C, H)\n\n        # Transformations\n        qkv = self.bn_qkv(self.qkv_transform(x))\n        q, k, v = torch.split(qkv.reshape(N * W, self.groups, self.group_planes * 2, H), [self.group_planes // 2, self.group_planes // 2, self.group_planes], dim=2)\n\n        # Calculate position embedding\n        all_embeddings = torch.index_select(self.relative, 1, self.flatten_index).view(self.group_planes * 2, self.kernel_size, self.kernel_size)\n        q_embedding, k_embedding, v_embedding = torch.split(all_embeddings, [self.group_planes // 2, self.group_planes // 2, self.group_planes], dim=0)\n        \n        qr = torch.einsum('bgci,cij->bgij', q, q_embedding)\n        kr = torch.einsum('bgci,cij->bgij', k, k_embedding).transpose(2, 3)\n        \n        qk = torch.einsum('bgci, bgcj->bgij', q, k)\n        \n        stacked_similarity = torch.cat([qk, qr, kr], dim=1)\n        stacked_similarity = self.bn_similarity(stacked_similarity).view(N * W, 3, self.groups, H, H).sum(dim=1)\n        #stacked_similarity = self.bn_qr(qr) + self.bn_kr(kr) + self.bn_qk(qk)\n        # (N, groups, H, H, W)\n        similarity = F.softmax(stacked_similarity, dim=3)\n        sv = torch.einsum('bgij,bgcj->bgci', similarity, v)\n        sve = torch.einsum('bgij,cij->bgci', similarity, v_embedding)\n        stacked_output = torch.cat([sv, sve], dim=-1).view(N * W, self.out_planes * 2, H)\n        output = self.bn_output(stacked_output).view(N, W, self.out_planes, 2, H).sum(dim=-2)\n\n        if self.width:\n            output = output.permute(0, 2, 1, 3)\n        else:\n            output = output.permute(0, 2, 3, 1)\n\n        if self.stride > 1:\n            output = self.pooling(output)\n\n        return output\n\n    def reset_parameters(self):\n        self.qkv_transform.weight.data.normal_(0, math.sqrt(1. / self.in_planes))\n        #nn.init.uniform_(self.relative, -0.1, 0.1)\n        nn.init.normal_(self.relative, 0., math.sqrt(1. / self.group_planes))\n\nclass AxialAttention_dynamic(nn.Module):\n    def __init__(self, in_planes, out_planes, groups=8, kernel_size=56,\n                 stride=1, bias=False, width=False):\n        assert (in_planes % groups == 0) and (out_planes % groups == 0)\n        super(AxialAttention_dynamic, self).__init__()\n        self.in_planes = in_planes\n        self.out_planes = out_planes\n        self.groups = groups\n        self.group_planes = out_planes // groups\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.bias = bias\n        self.width = width\n\n        # Multi-head self attention\n        self.qkv_transform = qkv_transform(in_planes, out_planes * 2, kernel_size=1, stride=1,\n                                           padding=0, bias=False)\n        self.bn_qkv = nn.BatchNorm1d(out_planes * 2)\n        self.bn_similarity = nn.BatchNorm2d(groups * 3)\n        self.bn_output = nn.BatchNorm1d(out_planes * 2)\n\n        # Priority on encoding\n\n        ## Initial values \n\n        self.f_qr = nn.Parameter(torch.tensor(0.1),  requires_grad=False) \n        self.f_kr = nn.Parameter(torch.tensor(0.1),  requires_grad=False)\n        self.f_sve = nn.Parameter(torch.tensor(0.1),  requires_grad=False)\n        self.f_sv = nn.Parameter(torch.tensor(1.0),  requires_grad=False)\n\n\n        # Position embedding\n        self.relative = nn.Parameter(torch.randn(self.group_planes * 2, kernel_size * 2 - 1), requires_grad=True)\n        query_index = torch.arange(kernel_size).unsqueeze(0)\n        key_index = torch.arange(kernel_size).unsqueeze(1)\n        relative_index = key_index - query_index + kernel_size - 1\n        self.register_buffer('flatten_index', relative_index.view(-1))\n        if stride > 1:\n            self.pooling = nn.AvgPool2d(stride, stride=stride)\n\n        self.reset_parameters()\n        # self.print_para()\n\n    def forward(self, x):\n        if self.width:\n            x = x.permute(0, 2, 1, 3)\n        else:\n            x = x.permute(0, 3, 1, 2)  # N, W, C, H\n        N, W, C, H = x.shape\n        x = x.contiguous().view(N * W, C, H)\n\n        # Transformations\n        qkv = self.bn_qkv(self.qkv_transform(x))\n        q, k, v = torch.split(qkv.reshape(N * W, self.groups, self.group_planes * 2, H), [self.group_planes // 2, self.group_planes // 2, self.group_planes], dim=2)\n\n        # Calculate position embedding\n        all_embeddings = torch.index_select(self.relative, 1, self.flatten_index).view(self.group_planes * 2, self.kernel_size, self.kernel_size)\n        q_embedding, k_embedding, v_embedding = torch.split(all_embeddings, [self.group_planes // 2, self.group_planes // 2, self.group_planes], dim=0)\n        qr = torch.einsum('bgci,cij->bgij', q, q_embedding)\n        kr = torch.einsum('bgci,cij->bgij', k, k_embedding).transpose(2, 3)\n        qk = torch.einsum('bgci, bgcj->bgij', q, k)\n\n\n        # multiply by factors\n        qr = torch.mul(qr, self.f_qr)\n        kr = torch.mul(kr, self.f_kr)\n\n        stacked_similarity = torch.cat([qk, qr, kr], dim=1)\n        stacked_similarity = self.bn_similarity(stacked_similarity).view(N * W, 3, self.groups, H, H).sum(dim=1)\n        #stacked_similarity = self.bn_qr(qr) + self.bn_kr(kr) + self.bn_qk(qk)\n        # (N, groups, H, H, W)\n        similarity = F.softmax(stacked_similarity, dim=3)\n        sv = torch.einsum('bgij,bgcj->bgci', similarity, v)\n        sve = torch.einsum('bgij,cij->bgci', similarity, v_embedding)\n\n        # multiply by factors\n        sv = torch.mul(sv, self.f_sv)\n        sve = torch.mul(sve, self.f_sve)\n\n        stacked_output = torch.cat([sv, sve], dim=-1).view(N * W, self.out_planes * 2, H)\n        output = self.bn_output(stacked_output).view(N, W, self.out_planes, 2, H).sum(dim=-2)\n\n        if self.width:\n            output = output.permute(0, 2, 1, 3)\n        else:\n            output = output.permute(0, 2, 3, 1)\n\n        if self.stride > 1:\n            output = self.pooling(output)\n\n        return output\n    def reset_parameters(self):\n        self.qkv_transform.weight.data.normal_(0, math.sqrt(1. / self.in_planes))\n        #nn.init.uniform_(self.relative, -0.1, 0.1)\n        nn.init.normal_(self.relative, 0., math.sqrt(1. / self.group_planes))\n\nclass AxialAttention_wopos(nn.Module):\n    def __init__(self, in_planes, out_planes, groups=8, kernel_size=56,\n                 stride=1, bias=False, width=False):\n        assert (in_planes % groups == 0) and (out_planes % groups == 0)\n        super(AxialAttention_wopos, self).__init__()\n        self.in_planes = in_planes\n        self.out_planes = out_planes\n        self.groups = groups\n        self.group_planes = out_planes // groups\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.bias = bias\n        self.width = width\n\n        # Multi-head self attention\n        self.qkv_transform = qkv_transform(in_planes, out_planes * 2, kernel_size=1, stride=1,\n                                           padding=0, bias=False)\n        self.bn_qkv = nn.BatchNorm1d(out_planes * 2)\n        self.bn_similarity = nn.BatchNorm2d(groups )\n\n        self.bn_output = nn.BatchNorm1d(out_planes * 1)\n\n        if stride > 1:\n            self.pooling = nn.AvgPool2d(stride, stride=stride)\n\n        self.reset_parameters()\n\n    def forward(self, x):\n        if self.width:\n            x = x.permute(0, 2, 1, 3)\n        else:\n            x = x.permute(0, 3, 1, 2)  # N, W, C, H\n        N, W, C, H = x.shape\n        x = x.contiguous().view(N * W, C, H)\n\n        # Transformations\n        qkv = self.bn_qkv(self.qkv_transform(x))\n        q, k, v = torch.split(qkv.reshape(N * W, self.groups, self.group_planes * 2, H), [self.group_planes // 2, self.group_planes // 2, self.group_planes], dim=2)\n\n        qk = torch.einsum('bgci, bgcj->bgij', q, k)\n\n        stacked_similarity = self.bn_similarity(qk).reshape(N * W, 1, self.groups, H, H).sum(dim=1).contiguous()\n\n        similarity = F.softmax(stacked_similarity, dim=3)\n        sv = torch.einsum('bgij,bgcj->bgci', similarity, v)\n\n        sv = sv.reshape(N*W,self.out_planes * 1, H).contiguous()\n        output = self.bn_output(sv).reshape(N, W, self.out_planes, 1, H).sum(dim=-2).contiguous()\n\n\n        if self.width:\n            output = output.permute(0, 2, 1, 3)\n        else:\n            output = output.permute(0, 2, 3, 1)\n\n        if self.stride > 1:\n            output = self.pooling(output)\n\n        return output\n\n    def reset_parameters(self):\n        self.qkv_transform.weight.data.normal_(0, math.sqrt(1. / self.in_planes))\n        #nn.init.uniform_(self.relative, -0.1, 0.1)\n        # nn.init.normal_(self.relative, 0., math.sqrt(1. / self.group_planes))\n\n#end of attn definition\n\nclass AxialBlock(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None, kernel_size=56):\n        super(AxialBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.))\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv_down = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.hight_block = AxialAttention(width, width, groups=groups, kernel_size=kernel_size)\n        self.width_block = AxialAttention(width, width, groups=groups, kernel_size=kernel_size, stride=stride, width=True)\n        self.conv_up = conv1x1(width, planes * self.expansion)\n        self.bn2 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv_down(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        # print(out.shape)\n        out = self.hight_block(out)\n        out = self.width_block(out)\n        out = self.relu(out)\n\n        out = self.conv_up(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass AxialBlock_dynamic(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None, kernel_size=56):\n        super(AxialBlock_dynamic, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.))\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv_down = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.hight_block = AxialAttention_dynamic(width, width, groups=groups, kernel_size=kernel_size)\n        self.width_block = AxialAttention_dynamic(width, width, groups=groups, kernel_size=kernel_size, stride=stride, width=True)\n        self.conv_up = conv1x1(width, planes * self.expansion)\n        self.bn2 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv_down(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.hight_block(out)\n        out = self.width_block(out)\n        out = self.relu(out)\n\n        out = self.conv_up(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass AxialBlock_wopos(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None, kernel_size=56):\n        super(AxialBlock_wopos, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        # print(kernel_size)\n        width = int(planes * (base_width / 64.))\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv_down = conv1x1(inplanes, width)\n        self.conv1 = nn.Conv2d(width, width, kernel_size = 1)\n        self.bn1 = norm_layer(width)\n        self.hight_block = AxialAttention_wopos(width, width, groups=groups, kernel_size=kernel_size)\n        self.width_block = AxialAttention_wopos(width, width, groups=groups, kernel_size=kernel_size, stride=stride, width=True)\n        self.conv_up = conv1x1(width, planes * self.expansion)\n        self.bn2 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        # pdb.set_trace()\n\n        out = self.conv_down(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        # print(out.shape)\n        out = self.hight_block(out)\n        out = self.width_block(out)\n\n        out = self.relu(out)\n\n        out = self.conv_up(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\n#end of block definition\n\n\nclass ResAxialAttentionUNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=2, zero_init_residual=True,\n                 groups=8, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None, s=0.125, img_size = 128,imgchan = 3):\n        super(ResAxialAttentionUNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = int(64 * s)\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(imgchan, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.conv2 = nn.Conv2d(self.inplanes, 128, kernel_size=3, stride=1, padding=1, bias=False)\n        self.conv3 = nn.Conv2d(128, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.bn2 = norm_layer(128)\n        self.bn3 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, int(128 * s), layers[0], kernel_size= (img_size//2))\n        self.layer2 = self._make_layer(block, int(256 * s), layers[1], stride=2, kernel_size=(img_size//2),\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, int(512 * s), layers[2], stride=2, kernel_size=(img_size//4),\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, int(1024 * s), layers[3], stride=2, kernel_size=(img_size//8),\n                                       dilate=replace_stride_with_dilation[2])\n        \n        # Decoder\n        self.decoder1 = nn.Conv2d(int(1024 *2*s)      ,        int(1024*2*s), kernel_size=3, stride=2, padding=1)\n        self.decoder2 = nn.Conv2d(int(1024  *2*s)     , int(1024*s), kernel_size=3, stride=1, padding=1)\n        self.decoder3 = nn.Conv2d(int(1024*s),  int(512*s), kernel_size=3, stride=1, padding=1)\n        self.decoder4 = nn.Conv2d(int(512*s) ,  int(256*s), kernel_size=3, stride=1, padding=1)\n        self.decoder5 = nn.Conv2d(int(256*s) , int(128*s) , kernel_size=3, stride=1, padding=1)\n        self.adjust   = nn.Conv2d(int(128*s) , num_classes, kernel_size=1, stride=1, padding=0)\n        self.soft     = nn.Softmax(dim=1)\n\n\n    def _make_layer(self, block, planes, blocks, kernel_size=56, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, groups=self.groups,\n                            base_width=self.base_width, dilation=previous_dilation, \n                            norm_layer=norm_layer, kernel_size=kernel_size))\n        self.inplanes = planes * block.expansion\n        if stride != 1:\n            kernel_size = kernel_size // 2\n\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer, kernel_size=kernel_size))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x):\n        \n        # AxialAttention Encoder\n        # pdb.set_trace()\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x1 = self.layer1(x)\n\n        x2 = self.layer2(x1)\n        # print(x2.shape)\n        x3 = self.layer3(x2)\n        # print(x3.shape)\n        x4 = self.layer4(x3)\n\n        x = F.relu(F.interpolate(self.decoder1(x4), scale_factor=(2,2), mode ='bilinear'))\n        x = torch.add(x, x4)\n        x = F.relu(F.interpolate(self.decoder2(x) , scale_factor=(2,2), mode ='bilinear'))\n        x = torch.add(x, x3)\n        x = F.relu(F.interpolate(self.decoder3(x) , scale_factor=(2,2), mode ='bilinear'))\n        x = torch.add(x, x2)\n        x = F.relu(F.interpolate(self.decoder4(x) , scale_factor=(2,2), mode ='bilinear'))\n        x = torch.add(x, x1)\n        x = F.relu(F.interpolate(self.decoder5(x) , scale_factor=(2,2), mode ='bilinear'))\n        x = self.adjust(F.relu(x))\n        # pdb.set_trace()\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)\n\nclass medt_net(nn.Module):\n\n    def __init__(self, block, block_2, layers, num_classes=2, zero_init_residual=True,\n                 groups=8, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None, s=0.125, img_size = 128,imgchan = 3):\n        super(medt_net, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = int(64 * s)\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(imgchan, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.conv2 = nn.Conv2d(self.inplanes, 128, kernel_size=3, stride=1, padding=1, bias=False)\n        self.conv3 = nn.Conv2d(128, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.bn2 = norm_layer(128)\n        self.bn3 = norm_layer(self.inplanes)\n        # self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, int(128 * s), layers[0], kernel_size= (img_size//2))\n        self.layer2 = self._make_layer(block, int(256 * s), layers[1], stride=2, kernel_size=(img_size//2),\n                                       dilate=replace_stride_with_dilation[0])\n        # self.layer3 = self._make_layer(block, int(512 * s), layers[2], stride=2, kernel_size=(img_size//4),\n        #                                dilate=replace_stride_with_dilation[1])\n        # self.layer4 = self._make_layer(block, int(1024 * s), layers[3], stride=2, kernel_size=(img_size//8),\n        #                                dilate=replace_stride_with_dilation[2])\n        \n        # Decoder\n        # self.decoder1 = nn.Conv2d(int(1024 *2*s)      ,        int(1024*2*s), kernel_size=3, stride=2, padding=1)\n        # self.decoder2 = nn.Conv2d(int(1024  *2*s)     , int(1024*s), kernel_size=3, stride=1, padding=1)\n        # self.decoder3 = nn.Conv2d(int(1024*s),  int(512*s), kernel_size=3, stride=1, padding=1)\n        self.decoder4 = nn.Conv2d(int(512*s) ,  int(256*s), kernel_size=3, stride=1, padding=1)\n        self.decoder5 = nn.Conv2d(int(256*s) , int(128*s) , kernel_size=3, stride=1, padding=1)\n        self.adjust   = nn.Conv2d(int(128*s) , num_classes, kernel_size=1, stride=1, padding=0)\n        self.soft     = nn.Softmax(dim=1)\n\n\n        self.conv1_p = nn.Conv2d(imgchan, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.conv2_p = nn.Conv2d(self.inplanes,128, kernel_size=3, stride=1, padding=1,\n                               bias=False)\n        self.conv3_p = nn.Conv2d(128, self.inplanes, kernel_size=3, stride=1, padding=1,\n                               bias=False)\n        # self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1_p = norm_layer(self.inplanes)\n        self.bn2_p = norm_layer(128)\n        self.bn3_p = norm_layer(self.inplanes)\n\n        self.relu_p = nn.ReLU(inplace=True)\n\n        img_size_p = img_size // 4\n\n        self.layer1_p = self._make_layer(block_2, int(128 * s), layers[0], kernel_size= (img_size_p//2))\n        self.layer2_p = self._make_layer(block_2, int(256 * s), layers[1], stride=2, kernel_size=(img_size_p//2),\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3_p = self._make_layer(block_2, int(512 * s), layers[2], stride=2, kernel_size=(img_size_p//4),\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4_p = self._make_layer(block_2, int(1024 * s), layers[3], stride=2, kernel_size=(img_size_p//8),\n                                       dilate=replace_stride_with_dilation[2])\n        \n        # Decoder\n        self.decoder1_p = nn.Conv2d(int(1024 *2*s)      ,        int(1024*2*s), kernel_size=3, stride=2, padding=1)\n        self.decoder2_p = nn.Conv2d(int(1024  *2*s)     , int(1024*s), kernel_size=3, stride=1, padding=1)\n        self.decoder3_p = nn.Conv2d(int(1024*s),  int(512*s), kernel_size=3, stride=1, padding=1)\n        self.decoder4_p = nn.Conv2d(int(512*s) ,  int(256*s), kernel_size=3, stride=1, padding=1)\n        self.decoder5_p = nn.Conv2d(int(256*s) , int(128*s) , kernel_size=3, stride=1, padding=1)\n\n        self.decoderf = nn.Conv2d(int(128*s) , int(128*s) , kernel_size=3, stride=1, padding=1)\n        self.adjust_p   = nn.Conv2d(int(128*s) , num_classes, kernel_size=1, stride=1, padding=0)\n        self.soft_p     = nn.Softmax(dim=1)\n\n\n    def _make_layer(self, block, planes, blocks, kernel_size=56, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, groups=self.groups,\n                            base_width=self.base_width, dilation=previous_dilation, \n                            norm_layer=norm_layer, kernel_size=kernel_size))\n        self.inplanes = planes * block.expansion\n        if stride != 1:\n            kernel_size = kernel_size // 2\n\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer, kernel_size=kernel_size))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x):\n\n        xin = x.clone()\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        # x = F.max_pool2d(x,2,2)\n        x = self.relu(x)\n        \n        # x = self.maxpool(x)\n        # pdb.set_trace()\n        x1 = self.layer1(x)\n        # print(x1.shape)\n        x2 = self.layer2(x1)\n        # print(x2.shape)\n        # x3 = self.layer3(x2)\n        # # print(x3.shape)\n        # x4 = self.layer4(x3)\n        # # print(x4.shape)\n        # x = F.relu(F.interpolate(self.decoder1(x4), scale_factor=(2,2), mode ='bilinear'))\n        # x = torch.add(x, x4)\n        # x = F.relu(F.interpolate(self.decoder2(x4) , scale_factor=(2,2), mode ='bilinear'))\n        # x = torch.add(x, x3)\n        # x = F.relu(F.interpolate(self.decoder3(x3) , scale_factor=(2,2), mode ='bilinear'))\n        # x = torch.add(x, x2)\n        x = F.relu(F.interpolate(self.decoder4(x2) , scale_factor=(2,2), mode ='bilinear'))\n        x = torch.add(x, x1)\n        x = F.relu(F.interpolate(self.decoder5(x) , scale_factor=(2,2), mode ='bilinear'))\n        # print(x.shape)\n        \n        # end of full image training \n\n        # y_out = torch.ones((1,2,128,128))\n        x_loc = x.clone()\n        # x = F.relu(F.interpolate(self.decoder5(x) , scale_factor=(2,2), mode ='bilinear'))\n        #start \n        for i in range(0,4):\n            for j in range(0,4):\n\n                x_p = xin[:,:,32*i:32*(i+1),32*j:32*(j+1)]\n                # begin patch wise\n                x_p = self.conv1_p(x_p)\n                x_p = self.bn1_p(x_p)\n                # x = F.max_pool2d(x,2,2)\n                x_p = self.relu(x_p)\n\n                x_p = self.conv2_p(x_p)\n                x_p = self.bn2_p(x_p)\n                # x = F.max_pool2d(x,2,2)\n                x_p = self.relu(x_p)\n                x_p = self.conv3_p(x_p)\n                x_p = self.bn3_p(x_p)\n                # x = F.max_pool2d(x,2,2)\n                x_p = self.relu(x_p)\n                \n                # x = self.maxpool(x)\n                # pdb.set_trace()\n                x1_p = self.layer1_p(x_p)\n                # print(x1.shape)\n                x2_p = self.layer2_p(x1_p)\n                # print(x2.shape)\n                x3_p = self.layer3_p(x2_p)\n                # # print(x3.shape)\n                x4_p = self.layer4_p(x3_p)\n                \n                x_p = F.relu(F.interpolate(self.decoder1_p(x4_p), scale_factor=(2,2), mode ='bilinear'))\n                x_p = torch.add(x_p, x4_p)\n                x_p = F.relu(F.interpolate(self.decoder2_p(x_p) , scale_factor=(2,2), mode ='bilinear'))\n                x_p = torch.add(x_p, x3_p)\n                x_p = F.relu(F.interpolate(self.decoder3_p(x_p) , scale_factor=(2,2), mode ='bilinear'))\n                x_p = torch.add(x_p, x2_p)\n                x_p = F.relu(F.interpolate(self.decoder4_p(x_p) , scale_factor=(2,2), mode ='bilinear'))\n                x_p = torch.add(x_p, x1_p)\n                x_p = F.relu(F.interpolate(self.decoder5_p(x_p) , scale_factor=(2,2), mode ='bilinear'))\n                \n                x_loc[:,:,32*i:32*(i+1),32*j:32*(j+1)] = x_p\n\n        x = torch.add(x,x_loc)\n        x = F.relu(self.decoderf(x))\n        \n        x = self.adjust(F.relu(x))\n\n        # pdb.set_trace()\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)\n\n\ndef axialunet(pretrained=False, **kwargs):\n    model = ResAxialAttentionUNet(AxialBlock, [1, 2, 4, 1], s= 0.125, **kwargs)\n    return model\n\ndef gated(pretrained=False, **kwargs):\n    model = ResAxialAttentionUNet(AxialBlock_dynamic, [1, 2, 4, 1], s= 0.125, **kwargs)\n    return model\n\ndef MedT(pretrained=False, **kwargs):\n    model = medt_net(AxialBlock_dynamic,AxialBlock_wopos, [1, 2, 4, 1], s= 0.125,  **kwargs)\n    return model\n\ndef logo(pretrained=False, **kwargs):\n    model = medt_net(AxialBlock,AxialBlock, [1, 2, 4, 1], s= 0.125, **kwargs)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:32:56.641687Z","iopub.execute_input":"2023-12-11T14:32:56.642082Z","iopub.status.idle":"2023-12-11T14:32:56.973456Z","shell.execute_reply.started":"2023-12-11T14:32:56.642054Z","shell.execute_reply":"2023-12-11T14:32:56.971610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Medical Transformer Code ","metadata":{}},{"cell_type":"code","source":"import pdb\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport random\n# use this seed function to make sure the result is reproducible\ndef reset_seed():\n  torch.manual_seed(42)\n  random.seed(42)\n  torch.cuda.manual_seed(42)\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchvision import transforms\nfrom torch.utils.data.dataset import Dataset\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport os\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\nimport cv2\n\nclass QU_Dataset(Dataset):\n  def __init__(self, image_dir, mask_dir, transform=None, state=None):\n    self.image_dir = image_dir\n    self.mask_dir = mask_dir\n    self.transform = transform\n    self.images = os.listdir(image_dir)\n\n  def __len__(self):\n    return len(self.images)\n\n  def __getitem__(self, index):\n    img_path = os.path.join(self.image_dir, self.images[index])\n    mask_path = os.path.join(self.mask_dir, self.images[index]).replace(\".png\",\".png\")\n    image = np.array(Image.open(img_path).convert(\"RGB\"))\n    NEW_IMAGE_HEIGHT = 224\n    NEW_IMAGE_WIDTH = 224\n    image = cv2.resize(image, (NEW_IMAGE_WIDTH, NEW_IMAGE_HEIGHT))\n\n    mask = np.array(Image.open(mask_path))\n    mask[mask >= 1] = 1.0\n\n    if self.transform is not None:\n      augmentations = self.transform(image=image, mask=mask)\n      image = augmentations[\"image\"]\n      mask = augmentations[\"mask\"]\n    return image, mask\n\nIMAGE_HEIGHT = 224\nIMAGE_WIDTH = 224\n\ntrain_transform = A.Compose(\n    [\n        #A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Rotate(limit=35, p=1.0),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.Normalize(\n            mean = [0.0, 0.0, 0.0],\n            std = [1.0, 1.0, 1.0],\n            max_pixel_value = 255.0\n        ),\n        ToTensorV2(),\n    ])\nval_transform = A.Compose(\n    [\n        #A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Normalize(\n            mean = [0.0, 0.0, 0.0],\n            std = [1.0, 1.0, 1.0],\n            max_pixel_value = 255.0\n        ),\n        ToTensorV2(),\n    ])\n\nTRAIN_DIR = r'C:\\Users\\umaiskhan\\Desktop\\Project Files\\NIH dataset\\train_img'\nTRAIN_MASK = r'C:\\Users\\umaiskhan\\Desktop\\Project Files\\NIH dataset\\train_masks'\nTEST_DIR = r'C:\\Users\\umaiskhan\\Desktop\\Project Files\\NIH dataset\\val_img'\nTEST_MASK = r'C:\\Users\\umaiskhan\\Desktop\\Project Files\\NIH dataset\\val_masks'\n\ntrain_ds1 = QU_Dataset(TRAIN_DIR, TRAIN_MASK, transform=train_transform)\n#train_ds, val_ds = torch.utils.data.random_split(train_ds1,[55,5], generator=torch.Generator().manual_seed(42))\ntest_ds = QU_Dataset(TEST_DIR, TEST_MASK, transform=val_transform)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:19:50.502905Z","iopub.execute_input":"2023-12-11T11:19:50.503465Z","iopub.status.idle":"2023-12-11T11:19:56.485281Z","shell.execute_reply.started":"2023-12-11T11:19:50.503428Z","shell.execute_reply":"2023-12-11T11:19:56.482971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv1x1(in_planes, out_planes, stride=1):\n  return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\nclass qkv_transform(nn.Conv1d):\n  \"\"\"Conv1d for qkv_transform\"\"\"\n# define Medical Transformer architecture\nclass AxialAttention(nn.Module):\n\n  def __init__(self,\n               in_planes,\n               out_planes,\n               groups=8,\n               kernel_size=56,\n               stride=1,\n               bias=False,\n               width=False):\n    \"\"\"\n    args :\n      in_planes : d_q\n      out_planes : d_out\n      groups : number heads\n      kernel_size : size of memory block\n    \"\"\"\n    super().__init__()\n    self.in_planes = in_planes\n    self.out_planes = out_planes\n    self.groups = groups\n    self.group_planes = out_planes // groups\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.bias = bias\n    self.width = width\n\n    # Multi-head self attention\n    # d_q = out_planes // 2, d_out = out_planes (out_planes = 16) => number of\n    self.qkv_transform = qkv_transform(in_planes, out_planes*2, kernel_size=1, stride=1,\n                                       padding=0, bias=False)\n    self.bn_qkv = nn.BatchNorm1d(out_planes * 2) # shape : q,k : (out_planes, d_q)\n    self.bn_similarity = nn.BatchNorm2d(groups * 3)\n    self.bn_output = nn.BatchNorm1d(out_planes * 2)\n\n    # position embedding\n    # (2 * kernel_size - 1) position : -(kernel_size-1),...,0,...,(kernel_size-1)\n    # number of channels = channels of r_q + channels of r_k + channels of r_v\n    # group_planes * 2   =  group_planes//2 * 2 + group_planes (dim value + dim query + dim key)\n    # => dim of embedding position : group_planes * 2 * (kernel_size * 2 - 1)\n    self.relative = nn.Parameter(torch.randn(self.group_planes * 2, kernel_size * 2 - 1), requires_grad=True)\n    query_index = torch.arange(kernel_size).unsqueeze(0)\n    key_index = torch.arange(kernel_size).unsqueeze(1)\n    relative_index = key_index - query_index + kernel_size - 1\n    self.register_buffer('flatten_index', relative_index.view(-1))\n    if stride > 1:\n      self.pooling = nn.AvgPool2d(stride, stride=stride)\n\n    self.reset_paremeters()\n\n  def forward(self, x):\n    # axial attention width-axis\n    if self.width:\n      x = x.permute(0, 2, 1, 3) # N, H, C, W\n    else:\n      x = x.permute(0, 3, 1, 2) # N, W, C, H\n    N, W, C, H = x.shape\n    x = x.contiguous().view(N*W, C, H)\n    # Transformations\n    qkv = self.bn_qkv(self.qkv_transform(x))\n    #print(x.shape)\n    #print(self.qkv_transform(x).shape)\n    q, k, v = torch.split(qkv.reshape(N*W, self.groups, self.group_planes * 2, H),\n                          [self.group_planes // 2, self.group_planes // 2, self.group_planes], dim=2)\n\n    # Calculate position embedding\n    # self.flatten_index : shape (kernel_size * kernel_size)\n    all_embedding = torch.index_select(self.relative, 1, self.flatten_index).view(self.group_planes * 2, self.kernel_size, self.kernel_size)\n    q_embedding, k_embedding, v_embedding = torch.split(all_embedding, [self.group_planes // 2, self.group_planes // 2, self.group_planes], dim=0)\n    qqn = q_embedding[0].detach().cpu().numpy()\n    #plt.imshow(qqn)\n    kqn = k_embedding[0].detach().cpu().numpy()\n    #plt.imshow(kqn)\n    # q : shape (N*W, number heads, self.group_planes // 2 = d_q, H)\n    # k : shape (N*W, number heads, self.group_planes // 2 = d_q, H)\n    # v : shape (N*W, number heads, self.group_planes = 2 * d_q, H)\n    # q_embedding : shape (self.group_planes // 2, kernel_size, kernel_size)\n    # k_embedding : shape (self.group_planes // 2, kernel_size, kernel_size)\n    # v_embedding : shape (self.group_planes, kernel_size, kernel_size)\n    # qr : shape (N*W, number heads, kernel_size, kernel_size)\n    # kr : shape (N*W, number heads, kernel_size, kernel_size)\n    # why transpose(2, 3) -> because index of equal (2) Medical-Transform\n    # qk : shape\n    qr = torch.einsum('bgci,cij->bgij', q, q_embedding)\n    kr = torch.einsum('bgci,cij->bgij', k, k_embedding).transpose(2, 3)\n    qk = torch.einsum('bgci,bgcj->bgij', q, k)\n    # batchnorm each qr, qk, kr before sum\n    stacked_similarity = torch.cat([qk, qr, kr], dim=1)\n    stacked_similarity = self.bn_similarity(stacked_similarity).view(N*W, 3, self.groups, H, H).sum(dim=1)\n    # stacked_similarity = self.bn_qr(qr) + self.bn_kr(kr) + self.bn_qk(qk)\n    # (N*W, groups, H, H)\n    similarity = F.softmax(stacked_similarity, dim=3)\n    sv = torch.einsum('bgij,bgcj->bgci', similarity, v)\n    sve = torch.einsum('bgij,cij->bgci', similarity, v_embedding)\n    stacked_output = torch.cat([sv, sve], dim=-1).view(N*W, self.out_planes*2, H)\n    output = self.bn_output(stacked_output).view(N, W, self.out_planes, 2, H).sum(dim=-2)\n\n    if self.width:\n      output = output.permute(0, 2, 1, 3)\n    else:\n      output = output.permute(0, 2, 3, 1)\n\n    if self.stride > 1:\n      output = self.pooling(output)\n    return output\n\n  def reset_paremeters(self):\n    self.qkv_transform.weight.data.normal_(0, math.sqrt(1. / self.in_planes))\n    nn.init.normal_(self.relative, 0, math.sqrt(1. / self.group_planes))\n\nclass AxialAttention_dynamic(nn.Module):\n  def __init__(self,\n               in_planes,\n               out_planes,\n               groups=8,\n               kernel_size=56,\n               stride=1,\n               bias=False,\n               width=False):\n      assert (in_planes % groups == 0) and (out_planes % groups == 0)\n      super(AxialAttention_dynamic, self).__init__()\n      self.in_planes = in_planes\n      self.out_planes = out_planes\n      self.groups = groups\n      self.group_planes = out_planes // groups\n      self.kernel_size = kernel_size\n      self.stride = stride\n      self.bias = bias\n      self.width = width\n\n      # Multi-head self attention\n      self.qkv_transform = qkv_transform(in_planes, out_planes * 2, kernel_size=1, stride=1,\n                                          padding=0, bias=False)\n      self.bn_qkv = nn.BatchNorm1d(out_planes * 2)\n      self.bn_similarity = nn.BatchNorm2d(groups * 3)\n      self.bn_output = nn.BatchNorm1d(out_planes * 2)\n\n      # Priority on encoding\n\n      ## Initial values\n\n      self.f_qr = nn.Parameter(torch.tensor(0.1),  requires_grad=False)\n      self.f_kr = nn.Parameter(torch.tensor(0.1),  requires_grad=False)\n      self.f_sve = nn.Parameter(torch.tensor(0.1),  requires_grad=False)\n      self.f_sv = nn.Parameter(torch.tensor(1.0),  requires_grad=False)\n\n\n      # Position embedding\n      self.relative = nn.Parameter(torch.randn(self.group_planes * 2, kernel_size * 2 - 1), requires_grad=True)\n      query_index = torch.arange(kernel_size).unsqueeze(0)\n      key_index = torch.arange(kernel_size).unsqueeze(1)\n      relative_index = key_index - query_index + kernel_size - 1\n      self.register_buffer('flatten_index', relative_index.view(-1))\n      if stride > 1:\n          self.pooling = nn.AvgPool2d(stride, stride=stride)\n\n      self.reset_parameters()\n      # self.print_para()\n\n  def forward(self, x):\n      if self.width:\n          x = x.permute(0, 2, 1, 3)\n      else:\n          x = x.permute(0, 3, 1, 2)  # N, W, C, H\n      N, W, C, H = x.shape\n      x = x.contiguous().view(N * W, C, H)\n\n      # Transformations\n      qkv = self.bn_qkv(self.qkv_transform(x))\n      q, k, v = torch.split(qkv.reshape(N * W, self.groups, self.group_planes * 2, H), [self.group_planes // 2, self.group_planes // 2, self.group_planes], dim=2)\n\n      # Calculate position embedding\n      all_embeddings = torch.index_select(self.relative, 1, self.flatten_index).view(self.group_planes * 2, self.kernel_size, self.kernel_size)\n      q_embedding, k_embedding, v_embedding = torch.split(all_embeddings, [self.group_planes // 2, self.group_planes // 2, self.group_planes], dim=0)\n      qr = torch.einsum('bgci,cij->bgij', q, q_embedding)\n      kr = torch.einsum('bgci,cij->bgij', k, k_embedding).transpose(2, 3)\n      qk = torch.einsum('bgci, bgcj->bgij', q, k)\n\n\n      # multiply by factors\n      qr = torch.mul(qr, self.f_qr)\n      kr = torch.mul(kr, self.f_kr)\n\n      stacked_similarity = torch.cat([qk, qr, kr], dim=1)\n      stacked_similarity = self.bn_similarity(stacked_similarity).view(N * W, 3, self.groups, H, H).sum(dim=1)\n      #stacked_similarity = self.bn_qr(qr) + self.bn_kr(kr) + self.bn_qk(qk)\n      # (N, groups, H, H, W)\n      similarity = F.softmax(stacked_similarity, dim=3)\n      sv = torch.einsum('bgij,bgcj->bgci', similarity, v)\n      sve = torch.einsum('bgij,cij->bgci', similarity, v_embedding)\n\n      # multiply by factors\n      sv = torch.mul(sv, self.f_sv)\n      sve = torch.mul(sve, self.f_sve)\n\n      stacked_output = torch.cat([sv, sve], dim=-1).view(N * W, self.out_planes * 2, H)\n      output = self.bn_output(stacked_output).view(N, W, self.out_planes, 2, H).sum(dim=-2)\n\n      if self.width:\n          output = output.permute(0, 2, 1, 3)\n      else:\n          output = output.permute(0, 2, 3, 1)\n\n      if self.stride > 1:\n          output = self.pooling(output)\n\n      return output\n  def reset_parameters(self):\n      self.qkv_transform.weight.data.normal_(0, math.sqrt(1. / self.in_planes))\n      #nn.init.uniform_(self.relative, -0.1, 0.1)\n      nn.init.normal_(self.relative, 0., math.sqrt(1. / self.group_planes))\nclass AxialAttention_wopos(nn.Module):\n  def __init__(self, in_planes, out_planes, groups=8, kernel_size=56,\n                stride=1, bias=False, width=False):\n      assert (in_planes % groups == 0) and (out_planes % groups == 0)\n      super(AxialAttention_wopos, self).__init__()\n      self.in_planes = in_planes\n      self.out_planes = out_planes\n      self.groups = groups\n      self.group_planes = out_planes // groups\n      self.kernel_size = kernel_size\n      self.stride = stride\n      self.bias = bias\n      self.width = width\n\n      # Multi-head self attention\n      self.qkv_transform = qkv_transform(in_planes, out_planes * 2, kernel_size=1, stride=1,\n                                          padding=0, bias=False)\n      self.bn_qkv = nn.BatchNorm1d(out_planes * 2)\n      self.bn_similarity = nn.BatchNorm2d(groups )\n\n      self.bn_output = nn.BatchNorm1d(out_planes * 1)\n\n      if stride > 1:\n          self.pooling = nn.AvgPool2d(stride, stride=stride)\n\n      self.reset_parameters()\n\n  def forward(self, x):\n      if self.width:\n          x = x.permute(0, 2, 1, 3)\n      else:\n          x = x.permute(0, 3, 1, 2)  # N, W, C, H\n      N, W, C, H = x.shape\n      x = x.contiguous().view(N * W, C, H)\n\n      # Transformations\n      qkv = self.bn_qkv(self.qkv_transform(x))\n      q, k, v = torch.split(qkv.reshape(N * W, self.groups, self.group_planes * 2, H), [self.group_planes // 2, self.group_planes // 2, self.group_planes], dim=2)\n\n      qk = torch.einsum('bgci, bgcj->bgij', q, k)\n\n      stacked_similarity = self.bn_similarity(qk).reshape(N * W, 1, self.groups, H, H).sum(dim=1).contiguous()\n\n      similarity = F.softmax(stacked_similarity, dim=3)\n      sv = torch.einsum('bgij,bgcj->bgci', similarity, v)\n\n      sv = sv.reshape(N*W,self.out_planes * 1, H).contiguous()\n      output = self.bn_output(sv).reshape(N, W, self.out_planes, 1, H).sum(dim=-2).contiguous()\n\n\n      if self.width:\n          output = output.permute(0, 2, 1, 3)\n      else:\n          output = output.permute(0, 2, 3, 1)\n\n      if self.stride > 1:\n          output = self.pooling(output)\n\n      return output\n\n  def reset_parameters(self):\n      self.qkv_transform.weight.data.normal_(0, math.sqrt(1. / self.in_planes))\n      #nn.init.uniform_(self.relative, -0.1, 0.1)\n      # nn.init.normal_(self.relative, 0., math.sqrt(1. / self.group_planes))\nclass AxialBlock(nn.Module):\n  expansion = 2\n\n  def __init__(self,\n               inplanes,\n               planes,\n               stride=1,\n               downsample=None,\n               groups=1,\n               base_width=64,\n               dilation=1,\n               norm_layer=None,\n               kernel_size=56):\n    super().__init__()\n    if norm_layer is None:\n      norm_layer = nn.BatchNorm2d\n    width = int(planes * (base_width / 64.))\n    # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n    self.groups = groups\n    self.conv_down = conv1x1(inplanes, width)\n    self.bn1 = norm_layer(width)\n    self.hight_block = AxialAttention(width, width, groups=groups, kernel_size=kernel_size)\n    self.width_block = AxialAttention(width, width, groups=groups, kernel_size=kernel_size, width=True)\n    self.conv_up = conv1x1(width, planes*self.expansion)\n    self.bn2 = norm_layer(planes * self.expansion)\n    self.relu = nn.ReLU(inplace=True)\n    self.downsample = downsample\n    self.stride = stride\n\n  def forward(self, x):\n    identity = x\n\n    out = self.conv_down(x)\n    out = self.bn1(out)\n    out = self.relu(out)\n\n    out = self.hight_block(out)\n    out = self.width_block(out)\n    out = self.relu(out)\n\n    out = self.conv_up(out)\n    out = self.bn2(out)\n\n    if self.downsample is not None:\n      identity = self.downsample(x)\n\n    out += identity\n    out = self.relu(out)\n\n    return out\nclass AxialBlock_dynamic(nn.Module):\n  expansion = 2\n\n  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                base_width=64, dilation=1, norm_layer=None, kernel_size=56):\n      super(AxialBlock_dynamic, self).__init__()\n      if norm_layer is None:\n          norm_layer = nn.BatchNorm2d\n      width = int(planes * (base_width / 64.))\n      # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n      self.conv_down = conv1x1(inplanes, width)\n      self.bn1 = norm_layer(width)\n      self.hight_block = AxialAttention_dynamic(width, width, groups=groups, kernel_size=kernel_size)\n      self.width_block = AxialAttention_dynamic(width, width, groups=groups, kernel_size=kernel_size, stride=stride, width=True)\n      self.conv_up = conv1x1(width, planes * self.expansion)\n      self.bn2 = norm_layer(planes * self.expansion)\n      self.relu = nn.ReLU(inplace=True)\n      self.downsample = downsample\n      self.stride = stride\n\n  def forward(self, x):\n      identity = x\n\n      out = self.conv_down(x)\n      out = self.bn1(out)\n      out = self.relu(out)\n\n      out = self.hight_block(out)\n      out = self.width_block(out)\n      out = self.relu(out)\n\n      out = self.conv_up(out)\n      out = self.bn2(out)\n\n      if self.downsample is not None:\n          identity = self.downsample(x)\n      #print(out.shape)\n      #print(identity.shape)\n      out += identity\n      out = self.relu(out)\n\n      return out\nclass AxialBlock_wopos(nn.Module):\n  expansion = 2\n\n  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                base_width=64, dilation=1, norm_layer=None, kernel_size=56):\n      super(AxialBlock_wopos, self).__init__()\n      if norm_layer is None:\n          norm_layer = nn.BatchNorm2d\n      # print(kernel_size)\n      width = int(planes * (base_width / 64.))\n      # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n      self.conv_down = conv1x1(inplanes, width)\n      self.conv1 = nn.Conv2d(width, width, kernel_size = 1)\n      self.bn1 = norm_layer(width)\n      self.hight_block = AxialAttention_wopos(width, width, groups=groups, kernel_size=kernel_size)\n      self.width_block = AxialAttention_wopos(width, width, groups=groups, kernel_size=kernel_size, stride=stride, width=True)\n      self.conv_up = conv1x1(width, planes * self.expansion)\n      self.bn2 = norm_layer(planes * self.expansion)\n      self.relu = nn.ReLU(inplace=True)\n      self.downsample = downsample\n      self.stride = stride\n\n  def forward(self, x):\n      identity = x\n\n      # pdb.set_trace()\n\n      out = self.conv_down(x)\n      out = self.bn1(out)\n      out = self.relu(out)\n      # print(out.shape)\n      out = self.hight_block(out)\n      out = self.width_block(out)\n\n      out = self.relu(out)\n\n      out = self.conv_up(out)\n      out = self.bn2(out)\n\n      if self.downsample is not None:\n          identity = self.downsample(x)\n\n      out += identity\n      out = self.relu(out)\n\n      return out\nclass ResAxialAttentionUNet(nn.Module):\n\n  def __init__(self, block, layers, num_classes=2, zero_init_residual=True,\n                groups=8, width_per_group=64, replace_stride_with_dilation=None,\n                norm_layer=None, s=0.125, img_size = 128,imgchan = 3):\n      super(ResAxialAttentionUNet, self).__init__()\n      if norm_layer is None:\n          norm_layer = nn.BatchNorm2d\n      self._norm_layer = norm_layer\n\n      self.inplanes = int(64 * s)\n      self.dilation = 1\n      if replace_stride_with_dilation is None:\n          replace_stride_with_dilation = [False, False, False]\n      if len(replace_stride_with_dilation) != 3:\n          raise ValueError(\"replace_stride_with_dilation should be None \"\n                            \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n      self.groups = groups\n      self.base_width = width_per_group\n      self.conv1 = nn.Conv2d(imgchan, self.inplanes, kernel_size=7, stride=2, padding=3,\n                              bias=False)\n      self.conv2 = nn.Conv2d(self.inplanes, 128, kernel_size=3, stride=1, padding=1, bias=False)\n      self.conv3 = nn.Conv2d(128, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n      self.bn1 = norm_layer(self.inplanes)\n      self.bn2 = norm_layer(128)\n      self.bn3 = norm_layer(self.inplanes)\n      self.relu = nn.ReLU(inplace=True)\n      # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n      self.layer1 = self._make_layer(block, int(128 * s), layers[0], kernel_size= (img_size//2))\n      self.layer2 = self._make_layer(block, int(256 * s), layers[1], stride=2, kernel_size=(img_size//2),\n                                      dilate=replace_stride_with_dilation[0])\n      self.layer3 = self._make_layer(block, int(512 * s), layers[2], stride=2, kernel_size=(img_size//4),\n                                      dilate=replace_stride_with_dilation[1])\n      self.layer4 = self._make_layer(block, int(1024 * s), layers[3], stride=2, kernel_size=(img_size//8),\n                                      dilate=replace_stride_with_dilation[2])\n\n      # Decoder\n      self.decoder1 = nn.Conv2d(int(1024 *2*s)      ,        int(1024*2*s), kernel_size=3, stride=2, padding=1)\n      self.decoder2 = nn.Conv2d(int(1024  *2*s)     , int(1024*s), kernel_size=3, stride=1, padding=1)\n      self.decoder3 = nn.Conv2d(int(1024*s),  int(512*s), kernel_size=3, stride=1, padding=1)\n      self.decoder4 = nn.Conv2d(int(512*s) ,  int(256*s), kernel_size=3, stride=1, padding=1)\n      self.decoder5 = nn.Conv2d(int(256*s) , int(128*s) , kernel_size=3, stride=1, padding=1)\n      self.adjust   = nn.Conv2d(int(128*s) , num_classes, kernel_size=1, stride=1, padding=0)\n      self.soft     = nn.Softmax(dim=1)\n\n\n  def _make_layer(self, block, planes, blocks, kernel_size=56, stride=1, dilate=False):\n      norm_layer = self._norm_layer\n      downsample = None\n      previous_dilation = self.dilation\n      if dilate:\n          self.dilation *= stride\n          stride = 1\n      if stride != 1 or self.inplanes != planes * block.expansion:\n          downsample = nn.Sequential(\n              conv1x1(self.inplanes, planes * block.expansion, stride),\n              norm_layer(planes * block.expansion),\n          )\n\n      layers = []\n      layers.append(block(self.inplanes, planes, stride, downsample, groups=self.groups,\n                          base_width=self.base_width, dilation=previous_dilation,\n                          norm_layer=norm_layer, kernel_size=kernel_size))\n      self.inplanes = planes * block.expansion\n      if stride != 1:\n          kernel_size = kernel_size // 2\n\n      for _ in range(1, blocks):\n          layers.append(block(self.inplanes, planes, groups=self.groups,\n                              base_width=self.base_width, dilation=self.dilation,\n                              norm_layer=norm_layer, kernel_size=kernel_size))\n\n      return nn.Sequential(*layers)\n\n  def _forward_impl(self, x):\n\n      # AxialAttention Encoder\n      # pdb.set_trace()\n      x = self.conv1(x)\n      x = self.bn1(x)\n      x = self.relu(x)\n      x = self.conv2(x)\n      x = self.bn2(x)\n      x = self.relu(x)\n      x = self.conv3(x)\n      x = self.bn3(x)\n      x = self.relu(x)\n      #print(x.shape)\n      x1 = self.layer1(x)\n      #print(x.shape)\n      x2 = self.layer2(x1)\n      # print(x2.shape)\n      x3 = self.layer3(x2)\n      # print(x3.shape)\n      x4 = self.layer4(x3)\n\n      x = F.relu(F.interpolate(self.decoder1(x4), scale_factor=(2,2), mode ='bilinear'))\n      x = torch.add(x, x4)\n      x = F.relu(F.interpolate(self.decoder2(x) , scale_factor=(2,2), mode ='bilinear'))\n      x = torch.add(x, x3)\n      x = F.relu(F.interpolate(self.decoder3(x) , scale_factor=(2,2), mode ='bilinear'))\n      x = torch.add(x, x2)\n      x = F.relu(F.interpolate(self.decoder4(x) , scale_factor=(2,2), mode ='bilinear'))\n      x = torch.add(x, x1)\n      x = F.relu(F.interpolate(self.decoder5(x) , scale_factor=(2,2), mode ='bilinear'))\n      x = self.adjust(F.relu(x))\n      # pdb.set_trace()\n      return x\n\n  def forward(self, x):\n      return self._forward_impl(x)\nclass medt_net(nn.Module):\n\n  def __init__(self, block, block_2, layers, num_classes=2, zero_init_residual=True,\n              groups=8, width_per_group=64, replace_stride_with_dilation=None,\n              norm_layer=None, s=0.125, img_size = 128,imgchan = 3):\n    super(medt_net, self).__init__()\n    if norm_layer is None:\n        norm_layer = nn.BatchNorm2d\n    self._norm_layer = norm_layer\n\n    self.inplanes = int(64 * s)\n    self.dilation = 1\n    if replace_stride_with_dilation is None:\n        replace_stride_with_dilation = [False, False, False]\n    if len(replace_stride_with_dilation) != 3:\n        raise ValueError(\"replace_stride_with_dilation should be None \"\n                          \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n    self.groups = groups\n    self.base_width = width_per_group\n    self.conv1 = nn.Conv2d(imgchan, self.inplanes, kernel_size=7, stride=2, padding=3,\n                            bias=False)\n    self.conv2 = nn.Conv2d(self.inplanes, 128, kernel_size=3, stride=1, padding=1, bias=False)\n    self.conv3 = nn.Conv2d(128, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n    self.bn1 = norm_layer(self.inplanes)\n    self.bn2 = norm_layer(128)\n    self.bn3 = norm_layer(self.inplanes)\n    # self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n    self.bn1 = norm_layer(self.inplanes)\n    self.relu = nn.ReLU(inplace=True)\n    # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n    self.layer1 = self._make_layer(block, int(128 * s), layers[0], kernel_size= (img_size//2))\n    self.layer2 = self._make_layer(block, int(256 * s), layers[1], stride=2, kernel_size=(img_size//2),\n                                    dilate=replace_stride_with_dilation[0])\n    # self.layer3 = self._make_layer(block, int(512 * s), layers[2], stride=2, kernel_size=(img_size//4),\n    #                                dilate=replace_stride_with_dilation[1])\n    # self.layer4 = self._make_layer(block, int(1024 * s), layers[3], stride=2, kernel_size=(img_size//8),\n    #                                dilate=replace_stride_with_dilation[2])\n\n    # Decoder\n    # self.decoder1 = nn.Conv2d(int(1024 *2*s)      ,        int(1024*2*s), kernel_size=3, stride=2, padding=1)\n    # self.decoder2 = nn.Conv2d(int(1024  *2*s)     , int(1024*s), kernel_size=3, stride=1, padding=1)\n    # self.decoder3 = nn.Conv2d(int(1024*s),  int(512*s), kernel_size=3, stride=1, padding=1)\n    self.decoder4 = nn.Conv2d(int(512*s) ,  int(256*s), kernel_size=3, stride=1, padding=1)\n    self.decoder5 = nn.Conv2d(int(256*s) , int(128*s) , kernel_size=3, stride=1, padding=1)\n    self.adjust   = nn.Conv2d(int(128*s) , num_classes, kernel_size=1, stride=1, padding=0)\n    self.soft     = nn.Softmax(dim=1)\n\n\n    self.conv1_p = nn.Conv2d(imgchan, self.inplanes, kernel_size=7, stride=2, padding=3,\n                            bias=False)\n    self.conv2_p = nn.Conv2d(self.inplanes,128, kernel_size=3, stride=1, padding=1,\n                            bias=False)\n    self.conv3_p = nn.Conv2d(128, self.inplanes, kernel_size=3, stride=1, padding=1,\n                            bias=False)\n    # self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n    self.bn1_p = norm_layer(self.inplanes)\n    self.bn2_p = norm_layer(128)\n    self.bn3_p = norm_layer(self.inplanes)\n\n    self.relu_p = nn.ReLU(inplace=True)\n\n    img_size_p = img_size // 4\n\n    self.layer1_p = self._make_layer(block_2, int(128 * s), layers[0], kernel_size= (img_size_p//2))\n    self.layer2_p = self._make_layer(block_2, int(256 * s), layers[1], stride=2, kernel_size=(img_size_p//2),\n                                    dilate=replace_stride_with_dilation[0])\n    self.layer3_p = self._make_layer(block_2, int(512 * s), layers[2], stride=2, kernel_size=(img_size_p//4),\n                                    dilate=replace_stride_with_dilation[1])\n    self.layer4_p = self._make_layer(block_2, int(1024 * s), layers[3], stride=2, kernel_size=(img_size_p//8),\n                                    dilate=replace_stride_with_dilation[2])\n\n    # Decoder\n    self.decoder1_p = nn.Conv2d(int(1024 *2*s)      ,        int(1024*2*s), kernel_size=3, stride=2, padding=1)\n    self.decoder2_p = nn.Conv2d(int(1024  *2*s)     , int(1024*s), kernel_size=3, stride=1, padding=1)\n    self.decoder3_p = nn.Conv2d(int(1024*s),  int(512*s), kernel_size=3, stride=1, padding=1)\n    self.decoder4_p = nn.Conv2d(int(512*s) ,  int(256*s), kernel_size=3, stride=1, padding=1)\n    self.decoder5_p = nn.Conv2d(int(256*s) , int(128*s) , kernel_size=3, stride=1, padding=1)\n\n    self.decoderf = nn.Conv2d(int(128*s) , int(128*s) , kernel_size=3, stride=1, padding=1)\n    self.adjust_p   = nn.Conv2d(int(128*s) , num_classes, kernel_size=1, stride=1, padding=0)\n    self.soft_p     = nn.Softmax(dim=1)\n\n\n  def _make_layer(self, block, planes, blocks, kernel_size=56, stride=1, dilate=False):\n    norm_layer = self._norm_layer\n    downsample = None\n    previous_dilation = self.dilation\n    if dilate:\n        self.dilation *= stride\n        stride = 1\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            conv1x1(self.inplanes, planes * block.expansion, stride),\n            norm_layer(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample, groups=self.groups,\n                        base_width=self.base_width, dilation=previous_dilation,\n                        norm_layer=norm_layer, kernel_size=kernel_size))\n    self.inplanes = planes * block.expansion\n    if stride != 1:\n        kernel_size = kernel_size // 2\n\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes, groups=self.groups,\n                            base_width=self.base_width, dilation=self.dilation,\n                            norm_layer=norm_layer, kernel_size=kernel_size))\n\n    return nn.Sequential(*layers)\n\n  def _forward_impl(self, x):\n\n    xin = x.clone()\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.conv2(x)\n    x = self.bn2(x)\n    x = self.relu(x)\n    x = self.conv3(x)\n    x = self.bn3(x)\n    # x = F.max_pool2d(x,2,2)\n    x = self.relu(x)\n\n    # x = self.maxpool(x)\n    # pdb.set_trace()\n    x1 = self.layer1(x)\n    # print(x1.shape)\n    x2 = self.layer2(x1)\n    # print(x2.shape)\n    # x3 = self.layer3(x2)\n    # # print(x3.shape)\n    # x4 = self.layer4(x3)\n    # # print(x4.shape)\n    # x = F.relu(F.interpolate(self.decoder1(x4), scale_factor=(2,2), mode ='bilinear'))\n    # x = torch.add(x, x4)\n    # x = F.relu(F.interpolate(self.decoder2(x4) , scale_factor=(2,2), mode ='bilinear'))\n    # x = torch.add(x, x3)\n    # x = F.relu(F.interpolate(self.decoder3(x3) , scale_factor=(2,2), mode ='bilinear'))\n    # x = torch.add(x, x2)\n    x = F.relu(F.interpolate(self.decoder4(x2) , scale_factor=(2,2), mode ='bilinear'))\n    x = torch.add(x, x1)\n    x = F.relu(F.interpolate(self.decoder5(x) , scale_factor=(2,2), mode ='bilinear'))\n    # print(x.shape)\n\n    # end of full image training\n\n    # y_out = torch.ones((1,2,128,128))\n    x_loc = x.clone()\n    # x = F.relu(F.interpolate(self.decoder5(x) , scale_factor=(2,2), mode ='bilinear'))\n    #start\n    for i in range(0,4):\n        for j in range(0,4):\n\n            x_p = xin[:,:,32*i:32*(i+1),32*j:32*(j+1)]\n            # begin patch wise\n            x_p = self.conv1_p(x_p)\n            x_p = self.bn1_p(x_p)\n            # x = F.max_pool2d(x,2,2)\n            x_p = self.relu(x_p)\n\n            x_p = self.conv2_p(x_p)\n            x_p = self.bn2_p(x_p)\n            # x = F.max_pool2d(x,2,2)\n            x_p = self.relu(x_p)\n            x_p = self.conv3_p(x_p)\n            x_p = self.bn3_p(x_p)\n            # x = F.max_pool2d(x,2,2)\n            x_p = self.relu(x_p)\n\n            # x = self.maxpool(x)\n            # pdb.set_trace()\n            x1_p = self.layer1_p(x_p)\n            # print(x1.shape)\n            x2_p = self.layer2_p(x1_p)\n            # print(x2.shape)\n            x3_p = self.layer3_p(x2_p)\n            # # print(x3.shape)\n            x4_p = self.layer4_p(x3_p)\n\n            x_p = F.relu(F.interpolate(self.decoder1_p(x4_p), scale_factor=(2,2), mode ='bilinear'))\n            x_p = torch.add(x_p, x4_p)\n            x_p = F.relu(F.interpolate(self.decoder2_p(x_p) , scale_factor=(2,2), mode ='bilinear'))\n            x_p = torch.add(x_p, x3_p)\n            x_p = F.relu(F.interpolate(self.decoder3_p(x_p) , scale_factor=(2,2), mode ='bilinear'))\n            x_p = torch.add(x_p, x2_p)\n            x_p = F.relu(F.interpolate(self.decoder4_p(x_p) , scale_factor=(2,2), mode ='bilinear'))\n            x_p = torch.add(x_p, x1_p)\n            x_p = F.relu(F.interpolate(self.decoder5_p(x_p) , scale_factor=(2,2), mode ='bilinear'))\n\n            x_loc[:,:,32*i:32*(i+1),32*j:32*(j+1)] = x_p\n\n    x = torch.add(x,x_loc)\n    x = F.relu(self.decoderf(x))\n\n    x = self.adjust(F.relu(x))\n\n    # pdb.set_trace()\n    return x\n\n  def forward(self, x):\n    return self._forward_impl(x)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:19:01.020468Z","iopub.execute_input":"2023-12-11T11:19:01.021204Z","iopub.status.idle":"2023-12-11T11:19:01.568617Z","shell.execute_reply.started":"2023-12-11T11:19:01.021048Z","shell.execute_reply":"2023-12-11T11:19:01.566245Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mednet glass \nhttps://www.kaggle.com/code/umaiskhan19/mednet-glas-dataset/edit","metadata":{}},{"cell_type":"code","source":"import pdb\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport random\n# use this seed function to make sure the result is reproducible\ndef reset_seed():\n  torch.manual_seed(42)\n  random.seed(42)\n  torch.cuda.manual_seed(42)\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchvision import transforms\nfrom torch.utils.data.dataset import Dataset\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport os\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\nimport cv2\n\nclass QU_Dataset(Dataset):\n  def __init__(self, image_dir, mask_dir, transform=None, state=None):\n    self.image_dir = image_dir\n    self.mask_dir = mask_dir\n    self.transform = transform\n    self.images = os.listdir(image_dir)\n\n  def __len__(self):\n    return len(self.images)\n\n  def __getitem__(self, index):\n    img_path = os.path.join(self.image_dir, self.images[index])\n    mask_path = os.path.join(self.mask_dir, self.images[index]).replace(\".png\",\".png\")\n    image = np.array(Image.open(img_path).convert(\"RGB\"))\n    NEW_IMAGE_HEIGHT = 224\n    NEW_IMAGE_WIDTH = 224\n    image = cv2.resize(image, (NEW_IMAGE_WIDTH, NEW_IMAGE_HEIGHT))\n\n    mask = np.array(Image.open(mask_path))\n    mask[mask >= 1] = 1.0\n\n    if self.transform is not None:\n      augmentations = self.transform(image=image, mask=mask)\n      image = augmentations[\"image\"]\n      mask = augmentations[\"mask\"]\n    return image, mask\n\nIMAGE_HEIGHT = 224\nIMAGE_WIDTH = 224\n\ntrain_transform = A.Compose(\n    [\n        #A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Rotate(limit=35, p=1.0),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.Normalize(\n            mean = [0.0, 0.0, 0.0],\n            std = [1.0, 1.0, 1.0],\n            max_pixel_value = 255.0\n        ),\n        ToTensorV2(),\n    ])\nval_transform = A.Compose(\n    [\n        #A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Normalize(\n            mean = [0.0, 0.0, 0.0],\n            std = [1.0, 1.0, 1.0],\n            max_pixel_value = 255.0\n        ),\n        ToTensorV2(),\n    ])\n\nTRAIN_DIR = r'C:\\Users\\umaiskhan\\Desktop\\Project Files\\NIH dataset\\train_img'\nTRAIN_MASK = r'C:\\Users\\umaiskhan\\Desktop\\Project Files\\NIH dataset\\train_masks'\nTEST_DIR = r'C:\\Users\\umaiskhan\\Desktop\\Project Files\\NIH dataset\\val_img'\nTEST_MASK = r'C:\\Users\\umaiskhan\\Desktop\\Project Files\\NIH dataset\\val_masks'\n\ntrain_ds1 = QU_Dataset(TRAIN_DIR, TRAIN_MASK, transform=train_transform)\n#train_ds, val_ds = torch.utils.data.random_split(train_ds1,[55,5], generator=torch.Generator().manual_seed(42))\ntest_ds = QU_Dataset(TEST_DIR, TEST_MASK, transform=val_transform)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:35:24.136329Z","iopub.execute_input":"2023-12-11T13:35:24.136839Z","iopub.status.idle":"2023-12-11T13:35:24.143982Z","shell.execute_reply.started":"2023-12-11T13:35:24.136798Z","shell.execute_reply":"2023-12-11T13:35:24.142712Z"},"trusted":true},"execution_count":null,"outputs":[]}]}