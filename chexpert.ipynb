{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1618416,"sourceType":"datasetVersion","datasetId":955838},{"sourceId":2169393,"sourceType":"datasetVersion","datasetId":1302315}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport time\nimport sys\nimport csv\nimport cv2\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn.functional as tfunc\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataset import random_split\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom PIL import Image\nimport torch.nn.functional as func\nfrom sklearn.metrics import roc_auc_score\nimport sklearn.metrics as metrics\nimport random\nuse_gpu = torch.cuda.is_available()\n# Paths to the files with training, and validation sets.\n# Each file contains pairs (path to image, output vector)\npathFileTrain = '/kaggle/input/chexpert/CheXpert-v1.0-small/train.csv'\npathFileValid = '/kaggle/input/chexpert/CheXpert-v1.0-small/valid.csv'\n\n# Neural network parameters:\nnnIsTrained = False                 #pre-trained using ImageNet\nnnClassCount = 14                   #dimension of the output\n\n# Training settings: batch size, maximum number of epochs\ntrBatchSize = 64\ntrMaxEpoch = 5\n\n# Parameters related to image transforms: size of the down-scaled image, cropped image\nimgtransResize = (320, 320)\nimgtransCrop = 224\n\n# Class names\nclass_names = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', \n               'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n               'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n\nclass CheXpertDataSet(Dataset):\n    def __init__(self, image_list_file, transform=None, policy=\"ones\"):\n        \"\"\"\n        image_list_file: path to the file containing images with corresponding labels.\n        transform: optional transform to be applied on a sample.\n        Upolicy: name the policy with regard to the uncertain labels\n        \"\"\"\n        image_names = []\n        labels = []\n\n        with open(image_list_file, \"r\") as f:\n            csvReader = csv.reader(f)\n            next(csvReader, None)\n            k=0\n            for line in csvReader:\n                k+=1\n                image_name= line[0]\n                label = line[5:]\n                \n                for i in range(14):\n                    if label[i]:\n                        a = float(label[i])\n                        if a == 1:\n                            label[i] = 1\n                        elif a == -1:\n                            if policy == \"ones\":\n                                label[i] = 1\n                            elif policy == \"zeroes\":\n                                label[i] = 0\n                            else:\n                                label[i] = 0\n                        else:\n                            label[i] = 0\n                    else:\n                        label[i] = 0\n                        \n                image_names.append(image_name)\n                labels.append(label)\n\n        self.image_names = image_names\n        self.labels = labels\n        self.transform = transform\n\n    def __getitem__(self, index):\n        \"\"\"Take the index of item and returns the image and its labels\"\"\"\n        \n        image_name = self.image_names[index]\n        image = Image.open(image_name).convert('RGB')\n        label = self.labels[index]\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, torch.FloatTensor(label)\n\n    def __len__(self):\n        return len(self.image_names)\n    \n    \n# normalize\nnormalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\ntransformList = []\n#transformList.append(transforms.Resize(imgtransCrop))\ntransformList.append(transforms.RandomResizedCrop(imgtransCrop))\ntransformList.append(transforms.RandomHorizontalFlip())\ntransformList.append(transforms.ToTensor())\ntransformList.append(normalize)      \ntransformSequence=transforms.Compose(transformList)\n\ndataset = CheXpertDataSet(pathFileTrain ,transformSequence, policy=\"ones\")\ndatasetTest, datasetTrain = random_split(dataset, [500, len(dataset) - 500])\ndatasetValid = CheXpertDataSet(pathFileValid, transformSequence)            \n\ndataLoaderTrain = DataLoader(dataset=datasetTrain, batch_size=trBatchSize, shuffle=True,  num_workers=24, pin_memory=True)\ndataLoaderVal = DataLoader(dataset=datasetValid, batch_size=trBatchSize, shuffle=False, num_workers=24, pin_memory=True)\ndataLoaderTest = DataLoader(dataset=datasetTest, num_workers=24, pin_memory=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-10T16:37:57.783664Z","iopub.execute_input":"2023-12-10T16:37:57.784820Z","iopub.status.idle":"2023-12-10T16:37:59.800655Z","shell.execute_reply.started":"2023-12-10T16:37:57.784775Z","shell.execute_reply":"2023-12-10T16:37:59.799691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CheXpertTrainer():\n\n    def train (model, dataLoaderTrain, dataLoaderVal, nnClassCount, trMaxEpoch, launchTimestamp, checkpoint):\n        \n        #SETTINGS: OPTIMIZER & SCHEDULER\n        optimizer = optim.Adam (model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n                \n        #SETTINGS: LOSS\n        loss = torch.nn.BCELoss(size_average = True)\n        \n        #LOAD CHECKPOINT \n        if checkpoint != None and use_gpu:\n            modelCheckpoint = torch.load(checkpoint)\n            model.load_state_dict(modelCheckpoint['state_dict'])\n            optimizer.load_state_dict(modelCheckpoint['optimizer'])\n\n        \n        #TRAIN THE NETWORK\n        lossMIN = 100000\n        \n        for epochID in range(0, trMaxEpoch):\n            \n            timestampTime = time.strftime(\"%H%M%S\")\n            timestampDate = time.strftime(\"%d%m%Y\")\n            timestampSTART = timestampDate + '-' + timestampTime\n            \n            batchs, losst, losse = CheXpertTrainer.epochTrain(model, dataLoaderTrain, optimizer, trMaxEpoch, nnClassCount, loss)\n            lossVal = CheXpertTrainer.epochVal(model, dataLoaderVal, optimizer, trMaxEpoch, nnClassCount, loss)\n\n\n            timestampTime = time.strftime(\"%H%M%S\")\n            timestampDate = time.strftime(\"%d%m%Y\")\n            timestampEND = timestampDate + '-' + timestampTime\n            \n            if lossVal < lossMIN:\n                lossMIN = lossVal    \n                torch.save({'epoch': epochID + 1, 'state_dict': model.state_dict(), 'best_loss': lossMIN, 'optimizer' : optimizer.state_dict()}, 'm-epoch'+str(epochID)+'-' + launchTimestamp + '.pth.tar')\n                print ('Epoch [' + str(epochID + 1) + '] [save] [' + timestampEND + '] loss= ' + str(lossVal))\n            else:\n                print ('Epoch [' + str(epochID + 1) + '] [----] [' + timestampEND + '] loss= ' + str(lossVal))\n        \n        return batchs, losst, losse        \n    #-------------------------------------------------------------------------------- \n       \n    def epochTrain(model, dataLoader, optimizer, epochMax, classCount, loss):\n        \n        batch = []\n        losstrain = []\n        losseval = []\n        \n        model.train()\n\n        for batchID, (varInput, target) in enumerate(dataLoaderTrain):\n            \n            varTarget = target.cuda(non_blocking = True)\n            \n            #varTarget = target.cuda()         \n\n\n            varOutput = model(varInput)\n            lossvalue = loss(varOutput, varTarget)\n                       \n            optimizer.zero_grad()\n            lossvalue.backward()\n            optimizer.step()\n            \n            l = lossvalue.item()\n            losstrain.append(l)\n            \n            if batchID%35==0:\n                print(batchID//35, \"% batches computed\")\n                #Fill three arrays to see the evolution of the loss\n\n\n                batch.append(batchID)\n                \n                le = CheXpertTrainer.epochVal(model, dataLoaderVal, optimizer, trMaxEpoch, nnClassCount, loss).item()\n                losseval.append(le)\n                \n                print(batchID)\n                print(l)\n                print(le)\n                \n        return batch, losstrain, losseval\n    \n    #-------------------------------------------------------------------------------- \n    \n    def epochVal(model, dataLoader, optimizer, epochMax, classCount, loss):\n        \n        model.eval()\n        \n        lossVal = 0\n        lossValNorm = 0\n\n        with torch.no_grad():\n            for i, (varInput, target) in enumerate(dataLoaderVal):\n                \n                target = target.cuda(non_blocking = True)\n                varOutput = model(varInput)\n                \n                losstensor = loss(varOutput, target)\n                lossVal += losstensor\n                lossValNorm += 1\n                \n        outLoss = lossVal / lossValNorm\n        return outLoss\n    \n    #--------------------------------------------------------------------------------     \n     \n    #---- Computes area under ROC curve \n    #---- dataGT - ground truth data\n    #---- dataPRED - predicted data\n    #---- classCount - number of classes\n    \n    def computeAUROC (dataGT, dataPRED, classCount):\n        \n        outAUROC = []\n        \n        datanpGT = dataGT.cpu().numpy()\n        datanpPRED = dataPRED.cpu().numpy()\n        \n        for i in range(classCount):\n            try:\n                outAUROC.append(roc_auc_score(datanpGT[:, i], datanpPRED[:, i]))\n            except ValueError:\n                pass\n        return outAUROC\n        \n        \n    #-------------------------------------------------------------------------------- \n    \n    \n    def test(model, dataLoaderTest, nnClassCount, checkpoint, class_names):   \n        \n        cudnn.benchmark = True\n        \n        if checkpoint != None and use_gpu:\n            modelCheckpoint = torch.load(checkpoint)\n            model.load_state_dict(modelCheckpoint['state_dict'])\n\n        if use_gpu:\n            outGT = torch.FloatTensor().cuda()\n            outPRED = torch.FloatTensor().cuda()\n        else:\n            outGT = torch.FloatTensor()\n            outPRED = torch.FloatTensor()\n       \n        model.eval()\n        \n        with torch.no_grad():\n            for i, (input, target) in enumerate(dataLoaderTest):\n\n                target = target.cuda()\n                outGT = torch.cat((outGT, target), 0).cuda()\n\n                bs, c, h, w = input.size()\n                varInput = input.view(-1, c, h, w)\n            \n                out = model(varInput)\n                outPRED = torch.cat((outPRED, out), 0)\n        aurocIndividual = CheXpertTrainer.computeAUROC(outGT, outPRED, nnClassCount)\n        aurocMean = np.array(aurocIndividual).mean()\n        \n        print ('AUROC mean ', aurocMean)\n        \n        for i in range (0, len(aurocIndividual)):\n            print (class_names[i], ' ', aurocIndividual[i])\n        \n        return outGT, outPRED\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T16:38:02.834067Z","iopub.execute_input":"2023-12-10T16:38:02.834463Z","iopub.status.idle":"2023-12-10T16:38:02.860670Z","shell.execute_reply.started":"2023-12-10T16:38:02.834433Z","shell.execute_reply":"2023-12-10T16:38:02.859714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# architectures","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\nclass DenseNet121(nn.Module):\n    \"\"\"Model modified.\n    The architecture of our model is the same as standard DenseNet121\n    except the classifier layer which has an additional sigmoid function.\n    \"\"\"\n    def __init__(self, out_size):\n        super(DenseNet121, self).__init__()\n        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n        num_ftrs = self.densenet121.classifier.in_features\n        self.densenet121.classifier = nn.Sequential(\n            nn.Linear(num_ftrs, out_size),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.densenet121(x)\n        return x\n  \nclass ResNet18(nn.Module):\n    def __init__(self, out_size):\n        super(ResNet18, self).__init__()\n        self.resnet18 = models.resnet18(pretrained=True)\n        num_ftrs = self.resnet18.fc.in_features\n        self.resnet18.fc = nn.Sequential(\n            nn.Linear(num_ftrs, out_size),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.resnet18(x)\n        return x\n\nclass InceptionV3(nn.Module):\n    def __init__(self, out_size):\n        super(InceptionV3, self).__init__()\n        self.inception_v3 = models.inception_v3(pretrained=True)\n        num_ftrs = self.inception_v3.fc.in_features\n        self.inception_v3.fc = nn.Sequential(\n            nn.Linear(num_ftrs, out_size),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.inception_v3(x)\n        return x\n# initialize and load the model\nmodel = ResNet18(nnClassCount).cuda()   # use .cuda()\nmodel = torch.nn.DataParallel(model).cuda() \n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T16:38:07.405966Z","iopub.execute_input":"2023-12-10T16:38:07.406623Z","iopub.status.idle":"2023-12-10T16:38:11.006000Z","shell.execute_reply.started":"2023-12-10T16:38:07.406585Z","shell.execute_reply":"2023-12-10T16:38:11.003734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"timestampTime = time.strftime(\"%H%M%S\")\ntimestampDate = time.strftime(\"%d%m%Y\")\ntimestampLaunch = timestampDate + '-' + timestampTime\nbatch, losst, losse = CheXpertTrainer.train(model, dataLoaderTrain, dataLoaderVal, nnClassCount, trMaxEpoch, timestampLaunch, checkpoint = None)\nprint(\"Model trained\")","metadata":{"execution":{"iopub.status.busy":"2023-12-10T16:38:11.008047Z","iopub.execute_input":"2023-12-10T16:38:11.008992Z","iopub.status.idle":"2023-12-10T16:38:11.933285Z","shell.execute_reply.started":"2023-12-10T16:38:11.008943Z","shell.execute_reply":"2023-12-10T16:38:11.931823Z"},"trusted":true},"execution_count":null,"outputs":[]}]}